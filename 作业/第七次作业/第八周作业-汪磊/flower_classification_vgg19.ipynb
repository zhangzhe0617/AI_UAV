{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg19 # tensorflow_vgg文件夹需要放在脚本当前目录下  \n",
    "from tensorflow_vgg import utils \n",
    "\n",
    "\"\"\"加载文件目录\"\"\"\n",
    "data_dir = r'E:\\USTC\\AI+UAV\\作业\\第八周\\flower_photos'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + '/'+each)]    # 读出文件夹中所有花的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\code\\python\\UAV_homework\\week8\\tensorflow_vgg\\vgg19.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project\\Anaconda3\\envs\\backup1\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"将数据穿过vgg19\"\"\"\n",
    "batch_size = 10\n",
    "codes_list = []\n",
    "labels = []   # 保存每一类图片对应的标签\n",
    "batch = []    # 保存每一类的图片\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg19.Vgg19() \n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])    # vgg的输入数据格式是244 * 224 * 3的像素数据\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)  # 建立vgg16模型\n",
    "\n",
    "    \"\"\"按类别分别计算relu层的输出\"\"\"\n",
    "    for each in classes:\n",
    "        class_path = data_dir + '/'+each\n",
    "        files = os.listdir(class_path)         # 读取每一类的图片\n",
    "        for i, file in enumerate(files, 1):    # enumerate返回索引和元素;第2个参数指定索引起始值\n",
    "            img = utils.load_image(os.path.join(class_path, file))    # os.path.join连接多个路径，这里获取了每一张图片的路径\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))               # 导入当前图片\n",
    "            labels.append(each)                                       # 导入当前标签\n",
    "            \n",
    "            if i % batch_size == 0 or i == len(files):    # 每10张图片是一个batch，进行批量处理\n",
    "                images = np.concatenate(batch)            # 将一个batch的图片连接成一个np数组\n",
    "\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict) # 将图片传入计算图,输出运行到relu6层(第一个全连接层后的激活层)的结果\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))       # 将所有的计算结果保存到codes中\n",
    "                \n",
    "                batch = []    # 准备处理下一个batch的数据\n",
    "                print('{} images processed'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(codes)    # vgg所有卷积层的输出，即全连接层的输入，是4096维的特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"将计算的结果写入磁盘文件\"\"\"\n",
    "with open('codes_vgg19', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "import csv\n",
    "with open('labels_vgg19', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"加载已有的数据\"\"\"\n",
    "import csv\n",
    "\n",
    "with open('labels_vgg19') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "    \n",
    "with open('codes_vgg19') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"对标签进行one-hot编码\"\"\"\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()    # 定义一个转换器，将标签二值化，默认正标签为1，负标签为0\n",
    "lb.fit(labels)\n",
    "labels_vecs = lb.transform(labels) # 执行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"样本分割\"\"\"\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)    # 定义一个分割器，分割的是索引值\n",
    "\n",
    "\"\"\"按比例对索引值进行分割\"\"\"\n",
    "train_idx, val_idx = next(ss.split(codes, labels))        # next函数返回可迭代对象的的下一个条目\n",
    "half_val_len = int(len(val_idx)/2)                  \n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]    # 测试数据分出一半为验证集，一半为测试集\n",
    "\n",
    "\"\"\"由索引值从样本中提取数据\"\"\"\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]\n",
    "\n",
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"添加全连接层\"\"\"\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])        # 输入特征的维度为none * 4096\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])    # 输入标签的维度为none * 5\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)    # 第一层256个节点\n",
    "    \n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)    # 第二层5个节点(输出层)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)    # 损失函数为交叉熵\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)    # 优化器为Adam\n",
    "\n",
    "predicted = tf.nn.softmax(logits)    # 模型输出\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))   \n",
    "# tf.argmax返回最大值的索引，第2个参数定义按列比较还是按行比较；\n",
    "# tf.equal()是对比两个矩阵相同位置的元素，相等就返回True，不等返回False\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"定义一个生成器：每次生成一个batch的训练集数据，大小为batch_size\"\"\"\n",
    "def get_batches(x, y, n_batches=10):\n",
    "    batch_size = len(x) // n_batches    # 结果向下取整\n",
    "    \n",
    "    for i in range(0, n_batches*batch_size, batch_size):\n",
    "        if i != (n_batches - 1) * batch_size:    # 不是最后一个batch\n",
    "            X, Y = x[i: i+batch_size], y[i: i+batch_size] \n",
    "        else:    # 若为最后一个batch，则读完剩余所有数据\n",
    "            X, Y = x[i:], y[i:]\n",
    "            \n",
    "        yield X, Y   # yield就是return一个值，并且记住这个返回的位置，下次迭代就从这个位置后开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"训练模型\"\"\"\n",
    "epochs = 10    # 对应n_batches\n",
    "iteration = 0\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            feed = {inputs_: x, labels_: y}\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs), \n",
    "                  \"Iteration: {}\".format(iteration), \n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:    # 训练集和验证集的比例为5:1\n",
    "                feed = {inputs_: val_x, labels_: val_y}\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "                \n",
    "    saver.save(sess, r'E:\\code\\python\\UAV_homework\\week8\\task1_result_vgg19\\flowers.ckpt')    # 非中文路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"测试模型\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(r'E:\\code\\python\\UAV_homework\\week8\\task1_result_vgg19'))\n",
    "    \n",
    "    feed = {inputs_: test_x, labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"读取待预测图片\"\"\"\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread\n",
    "\n",
    "test_img_path =r'E:\\USTC\\AI+UAV\\作业\\第八周\\rose.jpg'\n",
    "test_img = imread(test_img_path)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"使用模型进行预测\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    img = utils.load_image(test_img_path)\n",
    "    img = img.reshape((1, 224, 224, 3))\n",
    "\n",
    "    feed_dict = {input_: img}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(r'E:\\code\\python\\UAV_homework\\week8\\task1_result_vgg19'))\n",
    "    \n",
    "    feed = {inputs_: code}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"显示预测结果\"\"\"\n",
    "plt.barh(np.arange(5), prediction)\n",
    "_ = plt.yticks(np.arange(5), lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:backup1]",
   "language": "python",
   "name": "conda-env-backup1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
