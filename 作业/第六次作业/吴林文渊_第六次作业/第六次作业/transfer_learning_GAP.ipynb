{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def load_dataset(path):\n",
    "    data=load_files(path)#读取了整个文件夹 所以data是类似字典的数据类型\n",
    "    dog_files=np.array(data['filenames'])#返回所有目录下照片的文件名\n",
    "    dog_targets=to_categorical(np.array(data['target']),133)#返回0-132类别编号\n",
    "    return dog_files,dog_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files,train_targets=load_dataset(r'C:\\\\Users\\\\wlwy\\\\Documents\\\\dogImages\\\\dogImages\\\\train')\n",
    "valid_files,valid_targets=load_dataset(r'C:\\\\Users\\\\wlwy\\\\Documents\\\\dogImages\\\\dogImages\\valid')\n",
    "test_files,test_targets  =load_dataset(r'C:\\\\Users\\wlwy\\\\Documents\\\\dogImages\\\\dogImages\\\\test')\n",
    "\n",
    "dog_names=[item[25:-1] for item in glob('C:\\\\Users\\\\wlwy\\\\Documents\\\\dogImages\\\\dogImages\\\\train\\\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取之前经过训练好的数据，穿过了前面的Vgg16的多层  相当于冻结前面权重\n",
    "bottleneck_features=np.load('C:\\\\Users\\\\wlwy\\\\Documents\\\\DogVGG16Data.npz')\n",
    "train_vgg16=bottleneck_features['train']\n",
    "valid_vgg16=bottleneck_features['valid']\n",
    "test_vgg16 =bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))#增加了全局优化池GAP，用来减少参数，防止过拟合，即每7*7个方格中取一个\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/25\n",
      "6680/6680 [==============================] - 2s 233us/step - loss: 6.0034 - acc: 0.6102 - val_loss: 6.9562 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.95624, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 2/25\n",
      "6680/6680 [==============================] - 1s 212us/step - loss: 5.8675 - acc: 0.6208 - val_loss: 6.8331 - val_acc: 0.4862\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.95624 to 6.83307, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 3/25\n",
      "6680/6680 [==============================] - 1s 221us/step - loss: 5.6681 - acc: 0.6314 - val_loss: 6.7135 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.83307 to 6.71353, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 4/25\n",
      "6680/6680 [==============================] - 1s 218us/step - loss: 5.6011 - acc: 0.6406 - val_loss: 6.6686 - val_acc: 0.4850\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.71353 to 6.66863, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 5/25\n",
      "6680/6680 [==============================] - 2s 262us/step - loss: 5.5401 - acc: 0.6464 - val_loss: 6.5662 - val_acc: 0.4970\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.66863 to 6.56617, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 6/25\n",
      "6680/6680 [==============================] - 2s 267us/step - loss: 5.5070 - acc: 0.6515 - val_loss: 6.5459 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.56617 to 6.54590, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 7/25\n",
      "6680/6680 [==============================] - 2s 313us/step - loss: 5.4921 - acc: 0.6558 - val_loss: 6.5464 - val_acc: 0.5114\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.54590\n",
      "Epoch 8/25\n",
      "6680/6680 [==============================] - 2s 320us/step - loss: 5.4856 - acc: 0.6575 - val_loss: 6.5511 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.54590\n",
      "Epoch 9/25\n",
      "6680/6680 [==============================] - 2s 324us/step - loss: 5.4557 - acc: 0.6570 - val_loss: 6.4782 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.54590 to 6.47824, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 10/25\n",
      "6680/6680 [==============================] - 2s 265us/step - loss: 5.3791 - acc: 0.6602 - val_loss: 6.5239 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.47824\n",
      "Epoch 11/25\n",
      "6680/6680 [==============================] - 2s 298us/step - loss: 5.2890 - acc: 0.6650 - val_loss: 6.3994 - val_acc: 0.5234\n",
      "\n",
      "Epoch 00011: val_loss improved from 6.47824 to 6.39937, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 12/25\n",
      "6680/6680 [==============================] - 2s 264us/step - loss: 5.2002 - acc: 0.6699 - val_loss: 6.2606 - val_acc: 0.5186\n",
      "\n",
      "Epoch 00012: val_loss improved from 6.39937 to 6.26059, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 13/25\n",
      "6680/6680 [==============================] - 2s 282us/step - loss: 5.1508 - acc: 0.6754 - val_loss: 6.3226 - val_acc: 0.5329\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.26059\n",
      "Epoch 14/25\n",
      "6680/6680 [==============================] - 2s 260us/step - loss: 5.1040 - acc: 0.6784 - val_loss: 6.2372 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00014: val_loss improved from 6.26059 to 6.23721, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 15/25\n",
      "6680/6680 [==============================] - 2s 289us/step - loss: 5.0905 - acc: 0.6814 - val_loss: 6.2162 - val_acc: 0.5293\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.23721 to 6.21621, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 16/25\n",
      "6680/6680 [==============================] - 2s 281us/step - loss: 5.0819 - acc: 0.6816 - val_loss: 6.2686 - val_acc: 0.5269\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.21621\n",
      "Epoch 17/25\n",
      "6680/6680 [==============================] - 2s 291us/step - loss: 5.0657 - acc: 0.6820 - val_loss: 6.2769 - val_acc: 0.5317\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.21621\n",
      "Epoch 18/25\n",
      "6680/6680 [==============================] - 2s 294us/step - loss: 5.0189 - acc: 0.6826 - val_loss: 6.2097 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00018: val_loss improved from 6.21621 to 6.20967, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 19/25\n",
      "6680/6680 [==============================] - 2s 274us/step - loss: 4.9967 - acc: 0.6859 - val_loss: 6.1825 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00019: val_loss improved from 6.20967 to 6.18245, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 20/25\n",
      "6680/6680 [==============================] - 2s 307us/step - loss: 4.9811 - acc: 0.6876 - val_loss: 6.1968 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.18245\n",
      "Epoch 21/25\n",
      "6680/6680 [==============================] - 2s 304us/step - loss: 4.9459 - acc: 0.6867 - val_loss: 6.1741 - val_acc: 0.5269\n",
      "\n",
      "Epoch 00021: val_loss improved from 6.18245 to 6.17414, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 22/25\n",
      "6680/6680 [==============================] - 2s 336us/step - loss: 4.8294 - acc: 0.6904 - val_loss: 6.2194 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.17414\n",
      "Epoch 23/25\n",
      "6680/6680 [==============================] - 2s 350us/step - loss: 4.7939 - acc: 0.6961 - val_loss: 6.1274 - val_acc: 0.5174\n",
      "\n",
      "Epoch 00023: val_loss improved from 6.17414 to 6.12739, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 24/25\n",
      "6680/6680 [==============================] - 2s 330us/step - loss: 4.7777 - acc: 0.7001 - val_loss: 6.1023 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00024: val_loss improved from 6.12739 to 6.10232, saving model to dogvgg16.weights.best.hdf5\n",
      "Epoch 25/25\n",
      "6680/6680 [==============================] - 2s 321us/step - loss: 4.6904 - acc: 0.7003 - val_loss: 6.0226 - val_acc: 0.5353\n",
      "\n",
      "Epoch 00025: val_loss improved from 6.10232 to 6.02257, saving model to dogvgg16.weights.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ed2618208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='dogvgg16.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "model.fit(train_vgg16, train_targets, epochs=25, validation_data=(valid_vgg16, valid_targets), \n",
    "          callbacks=[checkpointer], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 54.7847%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('dogvgg16.weights.best.hdf5')\n",
    "vgg16_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) #argmax为取最大元素的索引值，即返回热独编码前的0-133标签编号\n",
    "                     for feature in test_vgg16]\n",
    "#vgg16_predictions得到每组数据对应的类别编号\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(vgg16_predictions)==\n",
    "                           np.argmax(test_targets, axis=1))/len(vgg16_predictions)\n",
    "print('\\nTest accuracy: %.4f%%' % test_accuracy)\n",
    "#精度提升  确实存在过拟合现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56,\n",
       " 69,\n",
       " 114,\n",
       " 18,\n",
       " 54,\n",
       " 92,\n",
       " 44,\n",
       " 51,\n",
       " 124,\n",
       " 50,\n",
       " 1,\n",
       " 18,\n",
       " 93,\n",
       " 132,\n",
       " 70,\n",
       " 44,\n",
       " 83,\n",
       " 68,\n",
       " 114,\n",
       " 41,\n",
       " 3,\n",
       " 68,\n",
       " 60,\n",
       " 95,\n",
       " 9,\n",
       " 130,\n",
       " 18,\n",
       " 18,\n",
       " 21,\n",
       " 112,\n",
       " 4,\n",
       " 108,\n",
       " 28,\n",
       " 11,\n",
       " 105,\n",
       " 44,\n",
       " 72,\n",
       " 38,\n",
       " 3,\n",
       " 117,\n",
       " 69,\n",
       " 84,\n",
       " 41,\n",
       " 26,\n",
       " 76,\n",
       " 1,\n",
       " 81,\n",
       " 89,\n",
       " 87,\n",
       " 7,\n",
       " 73,\n",
       " 119,\n",
       " 73,\n",
       " 115,\n",
       " 87,\n",
       " 99,\n",
       " 69,\n",
       " 103,\n",
       " 29,\n",
       " 2,\n",
       " 10,\n",
       " 102,\n",
       " 33,\n",
       " 95,\n",
       " 81,\n",
       " 27,\n",
       " 132,\n",
       " 95,\n",
       " 115,\n",
       " 37,\n",
       " 8,\n",
       " 57,\n",
       " 81,\n",
       " 69,\n",
       " 108,\n",
       " 76,\n",
       " 29,\n",
       " 130,\n",
       " 37,\n",
       " 95,\n",
       " 115,\n",
       " 102,\n",
       " 68,\n",
       " 93,\n",
       " 92,\n",
       " 106,\n",
       " 11,\n",
       " 45,\n",
       " 5,\n",
       " 84,\n",
       " 93,\n",
       " 14,\n",
       " 130,\n",
       " 101,\n",
       " 21,\n",
       " 21,\n",
       " 95,\n",
       " 101,\n",
       " 46,\n",
       " 83,\n",
       " 7,\n",
       " 10,\n",
       " 55,\n",
       " 81,\n",
       " 71,\n",
       " 114,\n",
       " 51,\n",
       " 56,\n",
       " 83,\n",
       " 86,\n",
       " 98,\n",
       " 62,\n",
       " 49,\n",
       " 11,\n",
       " 3,\n",
       " 10,\n",
       " 57,\n",
       " 82,\n",
       " 27,\n",
       " 43,\n",
       " 117,\n",
       " 0,\n",
       " 131,\n",
       " 98,\n",
       " 82,\n",
       " 109,\n",
       " 89,\n",
       " 78,\n",
       " 43,\n",
       " 28,\n",
       " 47,\n",
       " 117,\n",
       " 26,\n",
       " 109,\n",
       " 69,\n",
       " 1,\n",
       " 70,\n",
       " 55,\n",
       " 117,\n",
       " 49,\n",
       " 47,\n",
       " 74,\n",
       " 47,\n",
       " 55,\n",
       " 4,\n",
       " 2,\n",
       " 83,\n",
       " 43,\n",
       " 87,\n",
       " 6,\n",
       " 87,\n",
       " 57,\n",
       " 6,\n",
       " 82,\n",
       " 108,\n",
       " 1,\n",
       " 95,\n",
       " 9,\n",
       " 85,\n",
       " 112,\n",
       " 49,\n",
       " 28,\n",
       " 114,\n",
       " 87,\n",
       " 95,\n",
       " 37,\n",
       " 72,\n",
       " 21,\n",
       " 115,\n",
       " 116,\n",
       " 72,\n",
       " 120,\n",
       " 44,\n",
       " 59,\n",
       " 1,\n",
       " 62,\n",
       " 119,\n",
       " 1,\n",
       " 111,\n",
       " 87,\n",
       " 76,\n",
       " 99,\n",
       " 6,\n",
       " 6,\n",
       " 49,\n",
       " 121,\n",
       " 45,\n",
       " 6,\n",
       " 4,\n",
       " 47,\n",
       " 6,\n",
       " 47,\n",
       " 105,\n",
       " 123,\n",
       " 1,\n",
       " 45,\n",
       " 60,\n",
       " 10,\n",
       " 8,\n",
       " 102,\n",
       " 44,\n",
       " 50,\n",
       " 69,\n",
       " 76,\n",
       " 70,\n",
       " 89,\n",
       " 38,\n",
       " 4,\n",
       " 1,\n",
       " 84,\n",
       " 114,\n",
       " 14,\n",
       " 1,\n",
       " 21,\n",
       " 37,\n",
       " 62,\n",
       " 93,\n",
       " 49,\n",
       " 23,\n",
       " 60,\n",
       " 33,\n",
       " 117,\n",
       " 44,\n",
       " 102,\n",
       " 83,\n",
       " 120,\n",
       " 124,\n",
       " 70,\n",
       " 45,\n",
       " 124,\n",
       " 82,\n",
       " 62,\n",
       " 95,\n",
       " 43,\n",
       " 119,\n",
       " 32,\n",
       " 96,\n",
       " 62,\n",
       " 55,\n",
       " 104,\n",
       " 95,\n",
       " 47,\n",
       " 68,\n",
       " 56,\n",
       " 77,\n",
       " 78,\n",
       " 44,\n",
       " 33,\n",
       " 102,\n",
       " 52,\n",
       " 11,\n",
       " 32,\n",
       " 114,\n",
       " 63,\n",
       " 5,\n",
       " 57,\n",
       " 130,\n",
       " 10,\n",
       " 63,\n",
       " 26,\n",
       " 38,\n",
       " 62,\n",
       " 69,\n",
       " 41,\n",
       " 2,\n",
       " 86,\n",
       " 95,\n",
       " 33,\n",
       " 131,\n",
       " 4,\n",
       " 10,\n",
       " 41,\n",
       " 112,\n",
       " 95,\n",
       " 14,\n",
       " 95,\n",
       " 51,\n",
       " 12,\n",
       " 74,\n",
       " 117,\n",
       " 84,\n",
       " 28,\n",
       " 49,\n",
       " 21,\n",
       " 32,\n",
       " 92,\n",
       " 98,\n",
       " 111,\n",
       " 76,\n",
       " 103,\n",
       " 21,\n",
       " 57,\n",
       " 70,\n",
       " 59,\n",
       " 56,\n",
       " 55,\n",
       " 47,\n",
       " 68,\n",
       " 121,\n",
       " 21,\n",
       " 83,\n",
       " 98,\n",
       " 106,\n",
       " 46,\n",
       " 83,\n",
       " 76,\n",
       " 62,\n",
       " 73,\n",
       " 68,\n",
       " 6,\n",
       " 66,\n",
       " 29,\n",
       " 116,\n",
       " 76,\n",
       " 78,\n",
       " 112,\n",
       " 1,\n",
       " 33,\n",
       " 46,\n",
       " 0,\n",
       " 61,\n",
       " 55,\n",
       " 117,\n",
       " 41,\n",
       " 116,\n",
       " 41,\n",
       " 41,\n",
       " 6,\n",
       " 11,\n",
       " 111,\n",
       " 68,\n",
       " 87,\n",
       " 29,\n",
       " 121,\n",
       " 47,\n",
       " 28,\n",
       " 65,\n",
       " 124,\n",
       " 78,\n",
       " 98,\n",
       " 49,\n",
       " 78,\n",
       " 55,\n",
       " 68,\n",
       " 11,\n",
       " 23,\n",
       " 115,\n",
       " 43,\n",
       " 132,\n",
       " 83,\n",
       " 28,\n",
       " 73,\n",
       " 93,\n",
       " 12,\n",
       " 28,\n",
       " 47,\n",
       " 54,\n",
       " 66,\n",
       " 11,\n",
       " 76,\n",
       " 57,\n",
       " 74,\n",
       " 0,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 61,\n",
       " 14,\n",
       " 101,\n",
       " 69,\n",
       " 93,\n",
       " 30,\n",
       " 115,\n",
       " 44,\n",
       " 93,\n",
       " 117,\n",
       " 84,\n",
       " 14,\n",
       " 28,\n",
       " 78,\n",
       " 83,\n",
       " 98,\n",
       " 33,\n",
       " 132,\n",
       " 82,\n",
       " 99,\n",
       " 70,\n",
       " 2,\n",
       " 70,\n",
       " 50,\n",
       " 115,\n",
       " 30,\n",
       " 3,\n",
       " 59,\n",
       " 9,\n",
       " 69,\n",
       " 101,\n",
       " 55,\n",
       " 0,\n",
       " 2,\n",
       " 44,\n",
       " 48,\n",
       " 87,\n",
       " 26,\n",
       " 92,\n",
       " 102,\n",
       " 108,\n",
       " 56,\n",
       " 83,\n",
       " 93,\n",
       " 115,\n",
       " 49,\n",
       " 114,\n",
       " 11,\n",
       " 101,\n",
       " 55,\n",
       " 50,\n",
       " 81,\n",
       " 45,\n",
       " 45,\n",
       " 57,\n",
       " 93,\n",
       " 98,\n",
       " 87,\n",
       " 55,\n",
       " 56,\n",
       " 52,\n",
       " 81,\n",
       " 46,\n",
       " 49,\n",
       " 117,\n",
       " 114,\n",
       " 85,\n",
       " 14,\n",
       " 45,\n",
       " 111,\n",
       " 2,\n",
       " 130,\n",
       " 33,\n",
       " 81,\n",
       " 87,\n",
       " 73,\n",
       " 132,\n",
       " 105,\n",
       " 57,\n",
       " 85,\n",
       " 46,\n",
       " 63,\n",
       " 102,\n",
       " 69,\n",
       " 82,\n",
       " 0,\n",
       " 46,\n",
       " 59,\n",
       " 116,\n",
       " 66,\n",
       " 103,\n",
       " 57,\n",
       " 109,\n",
       " 106,\n",
       " 120,\n",
       " 76,\n",
       " 82,\n",
       " 48,\n",
       " 111,\n",
       " 28,\n",
       " 6,\n",
       " 4,\n",
       " 46,\n",
       " 130,\n",
       " 115,\n",
       " 57,\n",
       " 102,\n",
       " 71,\n",
       " 32,\n",
       " 4,\n",
       " 7,\n",
       " 55,\n",
       " 69,\n",
       " 57,\n",
       " 68,\n",
       " 27,\n",
       " 33,\n",
       " 21,\n",
       " 70,\n",
       " 46,\n",
       " 84,\n",
       " 117,\n",
       " 81,\n",
       " 4,\n",
       " 28,\n",
       " 1,\n",
       " 108,\n",
       " 61,\n",
       " 11,\n",
       " 4,\n",
       " 130,\n",
       " 26,\n",
       " 132,\n",
       " 43,\n",
       " 93,\n",
       " 44,\n",
       " 32,\n",
       " 125,\n",
       " 21,\n",
       " 112,\n",
       " 21,\n",
       " 45,\n",
       " 7,\n",
       " 102,\n",
       " 41,\n",
       " 119,\n",
       " 68,\n",
       " 96,\n",
       " 76,\n",
       " 84,\n",
       " 59,\n",
       " 86,\n",
       " 130,\n",
       " 29,\n",
       " 115,\n",
       " 38,\n",
       " 47,\n",
       " 81,\n",
       " 0,\n",
       " 87,\n",
       " 63,\n",
       " 78,\n",
       " 89,\n",
       " 111,\n",
       " 73,\n",
       " 114,\n",
       " 84,\n",
       " 38,\n",
       " 38,\n",
       " 60,\n",
       " 69,\n",
       " 38,\n",
       " 6,\n",
       " 105,\n",
       " 61,\n",
       " 78,\n",
       " 62,\n",
       " 92,\n",
       " 21,\n",
       " 28,\n",
       " 89,\n",
       " 99,\n",
       " 116,\n",
       " 26,\n",
       " 92,\n",
       " 76,\n",
       " 21,\n",
       " 14,\n",
       " 2,\n",
       " 96,\n",
       " 77,\n",
       " 44,\n",
       " 72,\n",
       " 70,\n",
       " 41,\n",
       " 55,\n",
       " 89,\n",
       " 106,\n",
       " 59,\n",
       " 11,\n",
       " 87,\n",
       " 78,\n",
       " 77,\n",
       " 130,\n",
       " 108,\n",
       " 111,\n",
       " 116,\n",
       " 99,\n",
       " 27,\n",
       " 48,\n",
       " 9,\n",
       " 130,\n",
       " 72,\n",
       " 46,\n",
       " 30,\n",
       " 93,\n",
       " 108,\n",
       " 49,\n",
       " 3,\n",
       " 14,\n",
       " 71,\n",
       " 93,\n",
       " 132,\n",
       " 23,\n",
       " 11,\n",
       " 47,\n",
       " 51,\n",
       " 43,\n",
       " 106,\n",
       " 93,\n",
       " 93,\n",
       " 124,\n",
       " 69,\n",
       " 61,\n",
       " 105,\n",
       " 87,\n",
       " 111,\n",
       " 7,\n",
       " 71,\n",
       " 131,\n",
       " 68,\n",
       " 4,\n",
       " 111,\n",
       " 5,\n",
       " 46,\n",
       " 7,\n",
       " 123,\n",
       " 26,\n",
       " 101,\n",
       " 116,\n",
       " 54,\n",
       " 49,\n",
       " 10,\n",
       " 54,\n",
       " 78,\n",
       " 117,\n",
       " 56,\n",
       " 114,\n",
       " 123,\n",
       " 105,\n",
       " 106,\n",
       " 76,\n",
       " 102,\n",
       " 89,\n",
       " 57,\n",
       " 10,\n",
       " 28,\n",
       " 32,\n",
       " 105,\n",
       " 104,\n",
       " 109,\n",
       " 44,\n",
       " 68,\n",
       " 9,\n",
       " 3,\n",
       " 52,\n",
       " 29,\n",
       " 63,\n",
       " 114,\n",
       " 55,\n",
       " 70,\n",
       " 28,\n",
       " 60,\n",
       " 78,\n",
       " 37,\n",
       " 106,\n",
       " 85,\n",
       " 98,\n",
       " 74,\n",
       " 68,\n",
       " 65,\n",
       " 83,\n",
       " 14,\n",
       " 121,\n",
       " 7,\n",
       " 85,\n",
       " 60,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 14,\n",
       " 70,\n",
       " 70,\n",
       " 55,\n",
       " 47,\n",
       " 45,\n",
       " 86,\n",
       " 18,\n",
       " 93,\n",
       " 70,\n",
       " 114,\n",
       " 33,\n",
       " 14,\n",
       " 56,\n",
       " 124,\n",
       " 2,\n",
       " 44,\n",
       " 102,\n",
       " 37,\n",
       " 30,\n",
       " 12,\n",
       " 72,\n",
       " 92,\n",
       " 38,\n",
       " 102,\n",
       " 85,\n",
       " 132,\n",
       " 52,\n",
       " 55,\n",
       " 50,\n",
       " 12,\n",
       " 89,\n",
       " 37,\n",
       " 82,\n",
       " 23,\n",
       " 4,\n",
       " 10,\n",
       " 41,\n",
       " 93,\n",
       " 5,\n",
       " 11,\n",
       " 4,\n",
       " 130,\n",
       " 7,\n",
       " 5,\n",
       " 56,\n",
       " 120,\n",
       " 28,\n",
       " 70,\n",
       " 85,\n",
       " 6,\n",
       " 82,\n",
       " 38,\n",
       " 51,\n",
       " 98,\n",
       " 59,\n",
       " 3,\n",
       " 41,\n",
       " 33,\n",
       " 105,\n",
       " 72,\n",
       " 52,\n",
       " 27,\n",
       " 50,\n",
       " 6,\n",
       " 48,\n",
       " 93,\n",
       " 33,\n",
       " 102,\n",
       " 41,\n",
       " 87,\n",
       " 26,\n",
       " 12,\n",
       " 125,\n",
       " 121,\n",
       " 11,\n",
       " 83,\n",
       " 41,\n",
       " 77,\n",
       " 51,\n",
       " 108,\n",
       " 30,\n",
       " 96,\n",
       " 18,\n",
       " 23,\n",
       " 99,\n",
       " 47,\n",
       " 3,\n",
       " 44,\n",
       " 27,\n",
       " 111,\n",
       " 21,\n",
       " 81,\n",
       " 103,\n",
       " 54,\n",
       " 12,\n",
       " 11,\n",
       " 59,\n",
       " 6,\n",
       " 33,\n",
       " 7,\n",
       " 43,\n",
       " 37,\n",
       " 116,\n",
       " 62,\n",
       " 5,\n",
       " 101,\n",
       " 111,\n",
       " 45,\n",
       " 37,\n",
       " 115,\n",
       " 68,\n",
       " 78,\n",
       " 6,\n",
       " 120,\n",
       " 112,\n",
       " 3,\n",
       " 26,\n",
       " 45,\n",
       " 121,\n",
       " 99,\n",
       " 70,\n",
       " 33,\n",
       " 89,\n",
       " 77,\n",
       " 77,\n",
       " 73,\n",
       " 103,\n",
       " 84,\n",
       " 68,\n",
       " 57,\n",
       " 70,\n",
       " 74,\n",
       " 123,\n",
       " 132,\n",
       " 23,\n",
       " 82,\n",
       " 93,\n",
       " 59,\n",
       " 106,\n",
       " 115,\n",
       " 14,\n",
       " 119,\n",
       " 9,\n",
       " 76,\n",
       " 81,\n",
       " 76,\n",
       " 11,\n",
       " 11,\n",
       " 78,\n",
       " 68,\n",
       " 105,\n",
       " 42,\n",
       " 54,\n",
       " 86,\n",
       " 65,\n",
       " 102,\n",
       " 86,\n",
       " 125,\n",
       " 93,\n",
       " 77,\n",
       " 109,\n",
       " 1,\n",
       " 49,\n",
       " 105,\n",
       " 116,\n",
       " 26,\n",
       " 114,\n",
       " 89,\n",
       " 101]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
