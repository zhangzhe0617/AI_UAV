本文档初稿时间为2018/11/1 周四凌晨，后续两天仍会继续进行神经网络的学习
【本文档用Notepad++打开效果会更好】

本周从单层感知机学起，熟悉了单层感知机的模型，实现方法，它能够处理的问题
    以及它的局限性；由此引出了二层感知机，即添加了一层隐藏层的感知机，用以
	处理非线性问题（如异或逻辑）。
	——————项目：SimplePerceptron利用单感知机模型实现了与、或、与非逻辑，并
	        利用双层感知机实现了异或逻辑（中间层其实是或和与非的组合，其输出
			作为最后一层与门的输入）;程序文件中有详细的注释。
			
    由多层感知机引出了神经网络，由于单层感知机使用阶跃函数（突变的）作为激活
	函数，所以给其添加了许多处理限制，而神经网络采用平滑连续的sigmoid函数等作为
	中间层的激活函数，使其可以处理许多线性和非线性问题。熟悉了sigmoid函数，ReLU
	函数，恒等函数和softmax函数，理解了softmax在处理多分类问题时的作用等等概念。
	——————项目：forward_neural_network简单实现了3层前向的神经网络，这里的权重和
	        偏置参数是人工设定好的，且不会进行更新，只是熟悉神经网络的内部实现细节，理解
	        它的内涵。其中，输出层的激活函数使用了恒等函数和softmax函数两种来获得输出值。
	        该神经网络的构造图在上传的<笔记拍照>文件夹中
	
	由于神经网络的权重参数空间很大，需要选取最优的参数来获得一个良好的预测模型，
	所以引出了神经网络<学习>的概念。引入了损失函数作为更新标准，学习了经常使用的
	损失函数，包括均方误差，交叉熵误差，softmax误差等，并用代码来实现；学习了梯度
	在神经网络中的重要性，能够用代码实现数值微分法求梯度，并用梯度下降法进行参数的
	更新；由于训练数据的庞大性，引出了mini-batch的学习方式，即每次在训练数据中随机
	选取小批量的数据用来训练模型。
	——————项目：Gradient_Descent包含自己实现四个.py文件（获取mnist数据集是现有的），
	        分别实现了数学意义上的梯度下降法（即梯度下降处理的是普通数学函数针对自变量
			的参数更新，求得函数的最小值）；functions.py实现了神经网络中要用到的函数，
			包括激活函数，损失函数，计算梯度的函数等；实现了一个3层神经网络的类，接着
			实现对该3层神经网络的训练（学习过程）。[文档中有详细说明]
			
【说明1：项目文件中的注释部分包含了在学习过程中做得笔记，自己的理解等】
【说明2：<笔记拍照>文件夹包含了学习过程中整理总结的手写版知识点的拍照图片】
【说明3：仍有一些问题清单，记录了学习过程中理解不透彻的点，需进一步深入了解】
【说明4：本周继续优化上周的统计飞行数据的代码，作了许多代码优化的调研，附在<笔记拍照>中，后续继续实现】
			
			