{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用神经网络来预测学生录取情况\n",
    "\n",
    "在该 notebook 中，应用神经网络并基于以下三项特征预测学生录取情况：\n",
    "\n",
    "- GRE 分数（测试）即 GRE Scores\n",
    "- GPA 分数（成绩）即 GPA Scores\n",
    "- 评级（1-4）即 Class rank (1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.加载数据集\n",
    "## 2.1. 从csv文件中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file into a pandas DataFrame\n",
    "df = pd.read_csv(\"student_data.csv\")\n",
    "\n",
    "# Print first 10 rows of df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 对rank进行one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = pd.get_dummies(df, columns=['rank'])\n",
    "one_hot_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩放数据\n",
    "由于gre数据范围为，gpa数据范围大概为，与其他数据的范围相差较大，不利于神经网络处理。因此，需要将gre和gpa两个特征数据缩放到0-1范围内，缩放方式为：减去最小值后除以最大值与最小值的差值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将DataFrame格式数据转成numpy数组，便于后续处理\n",
    "data = np.array(one_hot_data)\n",
    "\n",
    "# 数据缩放，统一处理\n",
    "data = (data - np.min(data, axis=0))/(np.max(data, axis=0)-np.min(data, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据分成训练集和测试集\n",
    "将数据集拆分成训练数据和测试数据，训练数据占70%，测试数据占30%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# 打乱数据顺序\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 获取训练数据集\n",
    "train_set = data[:280,:]\n",
    "\n",
    "# 获取测试数据集\n",
    "test_set = data[280:,:]\n",
    "\n",
    "print(data.shape)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据分成特征和标签\n",
    "将把数据分为特征 (features)（X）和标签 (labels)（y）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_set[:,1:]\n",
    "train_y = train_set[:,0]\n",
    "train_y = train_y.reshape(train_y.shape[0],1)\n",
    "test_X = test_set[:,1:]\n",
    "test_y = test_set[:,0]\n",
    "test_y = test_y.reshape(test_y.shape[0],1)\n",
    "\n",
    "# 转置\n",
    "train_X = train_X.T\n",
    "train_y = train_y.T\n",
    "test_X = test_X.T\n",
    "test_y = test_y.T\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        assert(parameters['W'+str(l)].shape==(layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b'+str(l)].shape==(layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([6,5,5,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Implements the relu activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    x -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    y -- output of relu(x), same shape as x\n",
    "    '''\n",
    "    \n",
    "    y = np.maximum(0, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    x -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    y -- output of sigmoid(x), same shape as x\n",
    "    '''\n",
    "\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_gradient(x):\n",
    "    '''\n",
    "    calculate the gradient for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    x -- where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    g -- Gradient of the RELU unit with respect to x\n",
    "    '''\n",
    "\n",
    "    g = np.array(x, copy=True)\n",
    "    g[x<=0] = 0\n",
    "    g[x>0] = 1\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    '''\n",
    "    calculate the gradient for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    x -- where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    g -- Gradient of the SIGMOID unit with respect to x\n",
    "    '''\n",
    "\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    g = s*(1-s)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-layer Forward propagation\n",
    "模型架构： [LINEAR -> RELU] × (L-1) -> LINEAR -> SIGMOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    if activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1,L): # 1~L-1层\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], 'relu')\n",
    "        caches.append(cache) # 缓存A_prev W b Z\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'sigmoid') # L层\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "实现交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, y):\n",
    "    '''\n",
    "    Implement the cross-entropy loss function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    J -- cross-entropy loss\n",
    "    '''\n",
    "\n",
    "    m = y.shape[1]\n",
    "    J = -(np.sum(y*np.log(AL)+(1-y)*np.log(1-AL), axis=1))/m\n",
    "    J = np.squeeze(J)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-layer Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    '''\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (A_prev, W, b, Z) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    '''\n",
    "\n",
    "    A_prev, W, b, Z = cache\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = dA * sigmoid_gradient(Z)\n",
    "    if activation == 'relu':\n",
    "        dZ = dA * relu_gradient(Z)\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward(AL, y, caches):\n",
    "    '''\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    y -- true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads['dA' + str(l)] = ... \n",
    "             grads['dW' + str(l)] = ...\n",
    "             grads['db' + str(l)] = ... \n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    dAL = -(y/AL-(1-y)/(1-AL))/m\n",
    "    grads['dA'+str(L)] = dAL\n",
    "    \n",
    "    grads['dA'+str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(dAL, caches[L-1], 'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA'+str(l)], grads['dW'+str(l+1)], grads['db'+str(l+1)] = linear_activation_backward(grads['dA'+str(l+1)], caches[l], 'relu')\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate * grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate * grads['db'+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [6, 12, 4, 1]\n",
    "\n",
    "def train_nn(X, y, layers_dims, learning_rate = 0.001, num_iterations = 500, print_cost=False):\n",
    "    \"\"\"\n",
    "    Train a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (6, number of examples)\n",
    "    y -- true \"label\" vector of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    # parameters initialization\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # forward propagation\n",
    "        AL, caches = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # computer cost\n",
    "        cost = compute_cost(AL, y)\n",
    "        \n",
    "        # backward propagation\n",
    "        grads = L_layer_backward(AL, y, caches)\n",
    "        \n",
    "        # update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # print the cost every 20 training example\n",
    "        if print_cost and i % 20 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    '''\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    '''\n",
    "    \n",
    "    AL, caches = L_layer_forward(X, parameters)\n",
    "    predictions = np.around(AL) # round off\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = train_nn(train_X, train_y, layers_dims, learning_rate = 0.01, num_iterations = 1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(test_X, parameters)\n",
    "accuracy = np.mean(preds == test_y)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
