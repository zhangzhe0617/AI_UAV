{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#下载ResNet-50\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "# 定义ResNet50模型\n",
    "ResNet50_model = ResNet50(weights='imagenet')#ResNet50是一个通过堆积残差快来实现深度增加不会导致精度下降的深度模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):#传进一个图片地址，生成一个四维数组\n",
    "    # 用PIL加载RGB图像为PIL.Image.Image类型\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # 将PIL.Image.Image类型转化为格式为(224, 224, 3)的3维张量\n",
    "    x = image.img_to_array(img)\n",
    "    # 将3维张量转化为格式为(1, 224, 224, 3)的4维张量并返回\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):#传入多个图片地址，生成4维数组\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#函数加载\n",
    "def load_dataset(path):\n",
    "    data=load_files(path)\n",
    "    dog_files=np.array(data['filenames'])\n",
    "    dog_targets=np_utils.to_categorical(np.array(data['target']),133)\n",
    "    return dog_files,dog_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "train_files,train_targets=load_dataset(r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\train')\n",
    "valid_files,valid_targets=load_dataset(r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\valid')\n",
    "test_files,test_targets=load_dataset(r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载狗品种列表\n",
    "dog_names=[item[72:-1] for item in sorted(glob(\"C:/Users/wlwy/Documents/four_task/dogImages/train/*/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_names= 133\n",
      "train_files= 6680\n",
      "valid_files= 835\n",
      "test_files= 836\n",
      "all number= 8351\n"
     ]
    }
   ],
   "source": [
    "print('dog_names=',len(dog_names))\n",
    "print('train_files=',len(train_files))\n",
    "print('valid_files=',len(valid_files))\n",
    "print('test_files=',len(test_files))\n",
    "print('all number=',len(train_files)+len(valid_files)+len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/wlwy/Documents/four_task/dogImages/train\\\\001.Affenpinscher\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\002.Afghan_hound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\003.Airedale_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\004.Akita\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\005.Alaskan_malamute\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\006.American_eskimo_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\007.American_foxhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\008.American_staffordshire_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\009.American_water_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\010.Anatolian_shepherd_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\011.Australian_cattle_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\012.Australian_shepherd\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\013.Australian_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\014.Basenji\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\015.Basset_hound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\016.Beagle\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\017.Bearded_collie\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\018.Beauceron\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\019.Bedlington_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\020.Belgian_malinois\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\021.Belgian_sheepdog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\022.Belgian_tervuren\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\023.Bernese_mountain_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\024.Bichon_frise\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\025.Black_and_tan_coonhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\026.Black_russian_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\027.Bloodhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\028.Bluetick_coonhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\029.Border_collie\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\030.Border_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\031.Borzoi\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\032.Boston_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\033.Bouvier_des_flandres\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\034.Boxer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\035.Boykin_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\036.Briard\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\037.Brittany\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\038.Brussels_griffon\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\039.Bull_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\040.Bulldog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\041.Bullmastiff\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\042.Cairn_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\043.Canaan_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\044.Cane_corso\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\045.Cardigan_welsh_corgi\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\046.Cavalier_king_charles_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\047.Chesapeake_bay_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\048.Chihuahua\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\049.Chinese_crested\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\050.Chinese_shar-pei\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\051.Chow_chow\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\052.Clumber_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\053.Cocker_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\054.Collie\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\055.Curly-coated_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\056.Dachshund\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\057.Dalmatian\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\058.Dandie_dinmont_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\059.Doberman_pinscher\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\060.Dogue_de_bordeaux\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\061.English_cocker_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\062.English_setter\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\063.English_springer_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\064.English_toy_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\065.Entlebucher_mountain_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\066.Field_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\067.Finnish_spitz\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\068.Flat-coated_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\069.French_bulldog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\070.German_pinscher\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\071.German_shepherd_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\072.German_shorthaired_pointer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\073.German_wirehaired_pointer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\074.Giant_schnauzer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\075.Glen_of_imaal_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\076.Golden_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\077.Gordon_setter\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\078.Great_dane\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\079.Great_pyrenees\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\080.Greater_swiss_mountain_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\081.Greyhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\082.Havanese\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\083.Ibizan_hound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\084.Icelandic_sheepdog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\085.Irish_red_and_white_setter\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\086.Irish_setter\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\087.Irish_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\088.Irish_water_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\089.Irish_wolfhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\090.Italian_greyhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\091.Japanese_chin\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\092.Keeshond\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\093.Kerry_blue_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\094.Komondor\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\095.Kuvasz\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\096.Labrador_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\097.Lakeland_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\098.Leonberger\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\099.Lhasa_apso\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\100.Lowchen\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\101.Maltese\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\102.Manchester_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\103.Mastiff\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\104.Miniature_schnauzer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\105.Neapolitan_mastiff\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\106.Newfoundland\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\107.Norfolk_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\108.Norwegian_buhund\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\109.Norwegian_elkhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\110.Norwegian_lundehund\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\111.Norwich_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\112.Nova_scotia_duck_tolling_retriever\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\113.Old_english_sheepdog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\114.Otterhound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\115.Papillon\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\116.Parson_russell_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\117.Pekingese\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\118.Pembroke_welsh_corgi\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\119.Petit_basset_griffon_vendeen\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\120.Pharaoh_hound\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\121.Plott\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\122.Pointer\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\123.Pomeranian\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\124.Poodle\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\125.Portuguese_water_dog\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\126.Saint_bernard\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\127.Silky_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\128.Smooth_fox_terrier\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\129.Tibetan_mastiff\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\130.Welsh_springer_spaniel\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\131.Wirehaired_pointing_griffon\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\132.Xoloitzcuintli\\\\',\n",
       " 'C:/Users/wlwy/Documents/four_task/dogImages/train\\\\133.Yorkshire_terrier\\\\']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob(\"C:/Users/wlwy/Documents/four_task/dogImages/train/*/\"))#进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Aaron_Eckhart\\\\Aaron_Eckhart_0001.jpg',\n",
       "       'C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Aaron_Guiel\\\\Aaron_Guiel_0001.jpg',\n",
       "       'C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Aaron_Patterson\\\\Aaron_Patterson_0001.jpg',\n",
       "       ...,\n",
       "       'C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Zumrati_Juma\\\\Zumrati_Juma_0001.jpg',\n",
       "       'C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Zurab_Tsereteli\\\\Zurab_Tsereteli_0001.jpg',\n",
       "       'C:/Users/wlwy/Documents/four_task/lfw/lfw\\\\Zydrunas_Ilgauskas\\\\Zydrunas_Ilgauskas_0001.jpg'],\n",
       "      dtype='<U122')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "#打乱后的人脸数据集的文件名\n",
    "human_files=np.array(glob(\"C:/Users/wlwy/Documents/four_task/lfw/lfw/*/*\"))\n",
    "human_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 68.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0.60784316, 0.61960787, 0.69411767],\n",
       "         [0.6627451 , 0.6509804 , 0.7254902 ],\n",
       "         [0.6392157 , 0.65882355, 0.7372549 ],\n",
       "         ...,\n",
       "         [0.5882353 , 0.6       , 0.6745098 ],\n",
       "         [0.57254905, 0.58431375, 0.65882355],\n",
       "         [0.5529412 , 0.5803922 , 0.6509804 ]],\n",
       "\n",
       "        [[0.6627451 , 0.67058825, 0.7294118 ],\n",
       "         [0.6392157 , 0.63529414, 0.69803923],\n",
       "         [0.6039216 , 0.6313726 , 0.7019608 ],\n",
       "         ...,\n",
       "         [0.54509807, 0.5803922 , 0.64705884],\n",
       "         [0.5568628 , 0.5764706 , 0.6509804 ],\n",
       "         [0.5411765 , 0.5686275 , 0.6392157 ]],\n",
       "\n",
       "        [[0.59607846, 0.6039216 , 0.6627451 ],\n",
       "         [0.64705884, 0.65882355, 0.7176471 ],\n",
       "         [0.6392157 , 0.6745098 , 0.7411765 ],\n",
       "         ...,\n",
       "         [0.5529412 , 0.6       , 0.654902  ],\n",
       "         [0.5529412 , 0.5803922 , 0.6509804 ],\n",
       "         [0.54509807, 0.57254905, 0.64705884]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.14509805, 0.13333334, 0.07450981],\n",
       "         [0.14117648, 0.13333334, 0.08627451],\n",
       "         [0.16470589, 0.14117648, 0.08627451],\n",
       "         ...,\n",
       "         [0.6431373 , 0.65882355, 0.7058824 ],\n",
       "         [0.6117647 , 0.61960787, 0.6784314 ],\n",
       "         [0.60784316, 0.6313726 , 0.69411767]],\n",
       "\n",
       "        [[0.14117648, 0.12941177, 0.05490196],\n",
       "         [0.15294118, 0.12941177, 0.06666667],\n",
       "         [0.15686275, 0.14117648, 0.09803922],\n",
       "         ...,\n",
       "         [0.5921569 , 0.6313726 , 0.6666667 ],\n",
       "         [0.5882353 , 0.5921569 , 0.6627451 ],\n",
       "         [0.6117647 , 0.62352943, 0.6901961 ]],\n",
       "\n",
       "        [[0.13333334, 0.12156863, 0.0627451 ],\n",
       "         [0.14117648, 0.11764706, 0.0627451 ],\n",
       "         [0.15294118, 0.13725491, 0.10196079],\n",
       "         ...,\n",
       "         [0.6039216 , 0.6431373 , 0.68235296],\n",
       "         [0.59607846, 0.6       , 0.67058825],\n",
       "         [0.60784316, 0.627451  , 0.7019608 ]]],\n",
       "\n",
       "\n",
       "       [[[0.6313726 , 0.38431373, 0.01960784],\n",
       "         [0.63529414, 0.3882353 , 0.02352941],\n",
       "         [0.6313726 , 0.38431373, 0.01176471],\n",
       "         ...,\n",
       "         [0.4745098 , 0.08627451, 0.00392157],\n",
       "         [0.44705883, 0.08627451, 0.        ],\n",
       "         [0.47058824, 0.10980392, 0.01960784]],\n",
       "\n",
       "        [[0.63529414, 0.3882353 , 0.02352941],\n",
       "         [0.6313726 , 0.38431373, 0.01960784],\n",
       "         [0.6392157 , 0.39215687, 0.01960784],\n",
       "         ...,\n",
       "         [0.4862745 , 0.09019608, 0.01176471],\n",
       "         [0.4509804 , 0.08235294, 0.        ],\n",
       "         [0.4745098 , 0.10588235, 0.00784314]],\n",
       "\n",
       "        [[0.63529414, 0.3882353 , 0.02352941],\n",
       "         [0.627451  , 0.38039216, 0.01568628],\n",
       "         [0.6431373 , 0.39607844, 0.02352941],\n",
       "         ...,\n",
       "         [0.4862745 , 0.09019608, 0.01176471],\n",
       "         [0.44313726, 0.06666667, 0.        ],\n",
       "         [0.4627451 , 0.08627451, 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.21176471, 0.10980392, 0.05098039],\n",
       "         [0.21568628, 0.11372549, 0.05490196],\n",
       "         [0.19607843, 0.10588235, 0.04313726],\n",
       "         ...,\n",
       "         [0.21568628, 0.10588235, 0.10196079],\n",
       "         [0.23921569, 0.11372549, 0.07058824],\n",
       "         [0.25490198, 0.12941177, 0.08627451]],\n",
       "\n",
       "        [[0.20392157, 0.10196079, 0.04313726],\n",
       "         [0.21176471, 0.10980392, 0.05098039],\n",
       "         [0.19607843, 0.10588235, 0.04313726],\n",
       "         ...,\n",
       "         [0.19607843, 0.08627451, 0.08235294],\n",
       "         [0.22352941, 0.09803922, 0.05490196],\n",
       "         [0.24313726, 0.11764706, 0.07450981]],\n",
       "\n",
       "        [[0.19607843, 0.09411765, 0.03529412],\n",
       "         [0.22352941, 0.12156863, 0.0627451 ],\n",
       "         [0.20392157, 0.11372549, 0.05098039],\n",
       "         ...,\n",
       "         [0.14117648, 0.04313726, 0.02745098],\n",
       "         [0.19215687, 0.07450981, 0.04313726],\n",
       "         [0.20784314, 0.09019608, 0.05882353]]],\n",
       "\n",
       "\n",
       "       [[[0.61960787, 0.62352943, 0.6039216 ],\n",
       "         [0.627451  , 0.6313726 , 0.6117647 ],\n",
       "         [0.627451  , 0.6313726 , 0.6117647 ],\n",
       "         ...,\n",
       "         [0.34901962, 0.3529412 , 0.32941177],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ]],\n",
       "\n",
       "        [[0.61960787, 0.62352943, 0.6039216 ],\n",
       "         [0.627451  , 0.6313726 , 0.6117647 ],\n",
       "         [0.6313726 , 0.63529414, 0.6156863 ],\n",
       "         ...,\n",
       "         [0.34901962, 0.3529412 , 0.32941177],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ]],\n",
       "\n",
       "        [[0.61960787, 0.62352943, 0.6039216 ],\n",
       "         [0.627451  , 0.6313726 , 0.6117647 ],\n",
       "         [0.63529414, 0.6392157 , 0.61960787],\n",
       "         ...,\n",
       "         [0.34901962, 0.3529412 , 0.32941177],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ],\n",
       "         [0.35686275, 0.36078432, 0.3372549 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.53333336, 0.40784314, 0.46666667],\n",
       "         [0.5372549 , 0.41568628, 0.47843137],\n",
       "         [0.5254902 , 0.4117647 , 0.48235294],\n",
       "         ...,\n",
       "         [0.5254902 , 0.40392157, 0.42745098],\n",
       "         [0.52156866, 0.4       , 0.43137255],\n",
       "         [0.5764706 , 0.4509804 , 0.49411765]],\n",
       "\n",
       "        [[0.2509804 , 0.19215687, 0.21176471],\n",
       "         [0.5254902 , 0.43137255, 0.4862745 ],\n",
       "         [0.5647059 , 0.44313726, 0.5137255 ],\n",
       "         ...,\n",
       "         [0.57254905, 0.44313726, 0.47058824],\n",
       "         [0.4862745 , 0.35686275, 0.38431373],\n",
       "         [0.56078434, 0.43137255, 0.45882353]],\n",
       "\n",
       "        [[0.1882353 , 0.20392157, 0.2       ],\n",
       "         [0.30588236, 0.26666668, 0.3019608 ],\n",
       "         [0.5058824 , 0.42352942, 0.48235294],\n",
       "         ...,\n",
       "         [0.6117647 , 0.48235294, 0.50980395],\n",
       "         [0.5294118 , 0.4       , 0.42745098],\n",
       "         [0.5568628 , 0.42745098, 0.45490196]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         ...,\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         ...,\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         ...,\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.73333335, 0.72156864, 0.7882353 ],\n",
       "         [0.8627451 , 0.8666667 , 0.84313726],\n",
       "         [0.9137255 , 0.9137255 , 0.94509804],\n",
       "         ...,\n",
       "         [0.7529412 , 0.7411765 , 0.6745098 ],\n",
       "         [0.3647059 , 0.3882353 , 0.33333334],\n",
       "         [0.16078432, 0.14509805, 0.14117648]],\n",
       "\n",
       "        [[0.6039216 , 0.61960787, 0.6627451 ],\n",
       "         [0.45490196, 0.43137255, 0.44705883],\n",
       "         [0.70980394, 0.7254902 , 0.77254903],\n",
       "         ...,\n",
       "         [0.27058825, 0.20784314, 0.21176471],\n",
       "         [0.48235294, 0.50980395, 0.47843137],\n",
       "         [0.99215686, 0.99607843, 1.        ]],\n",
       "\n",
       "        [[0.2627451 , 0.25490198, 0.20784314],\n",
       "         [0.43137255, 0.40392157, 0.38039216],\n",
       "         [0.30980393, 0.2901961 , 0.26666668],\n",
       "         ...,\n",
       "         [0.96862745, 1.        , 0.99215686],\n",
       "         [0.47843137, 0.48235294, 0.4627451 ],\n",
       "         [0.59607846, 0.57254905, 0.6117647 ]]],\n",
       "\n",
       "\n",
       "       [[[0.01176471, 0.01176471, 0.00392157],\n",
       "         [0.00784314, 0.00784314, 0.        ],\n",
       "         [0.04313726, 0.04313726, 0.03529412],\n",
       "         ...,\n",
       "         [0.05490196, 0.02352941, 0.01176471],\n",
       "         [0.05098039, 0.01960784, 0.00784314],\n",
       "         [0.05490196, 0.02352941, 0.01176471]],\n",
       "\n",
       "        [[0.01176471, 0.01176471, 0.00392157],\n",
       "         [0.01568628, 0.01568628, 0.00784314],\n",
       "         [0.01176471, 0.01176471, 0.00392157],\n",
       "         ...,\n",
       "         [0.04705882, 0.01568628, 0.00392157],\n",
       "         [0.04705882, 0.01568628, 0.00392157],\n",
       "         [0.04705882, 0.01568628, 0.00392157]],\n",
       "\n",
       "        [[0.02352941, 0.02352941, 0.01568628],\n",
       "         [0.01176471, 0.01176471, 0.00392157],\n",
       "         [0.02745098, 0.02745098, 0.01960784],\n",
       "         ...,\n",
       "         [0.05098039, 0.01960784, 0.00784314],\n",
       "         [0.05098039, 0.01960784, 0.00784314],\n",
       "         [0.05098039, 0.01960784, 0.00784314]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.98039216, 0.60784316, 0.69411767],\n",
       "         [0.96862745, 0.5882353 , 0.6666667 ],\n",
       "         [1.        , 0.6666667 , 0.72156864],\n",
       "         ...,\n",
       "         [0.25882354, 0.33333334, 0.4       ],\n",
       "         [0.25490198, 0.31764707, 0.38039216],\n",
       "         [0.24705882, 0.3019608 , 0.3529412 ]],\n",
       "\n",
       "        [[0.6392157 , 0.6117647 , 0.6431373 ],\n",
       "         [0.85490197, 0.7529412 , 0.8039216 ],\n",
       "         [0.7529412 , 0.6039216 , 0.654902  ],\n",
       "         ...,\n",
       "         [0.31764707, 0.36862746, 0.43529412],\n",
       "         [0.25490198, 0.3254902 , 0.38039216],\n",
       "         [0.23529412, 0.3254902 , 0.3882353 ]],\n",
       "\n",
       "        [[0.8       , 0.9137255 , 0.8980392 ],\n",
       "         [0.8235294 , 0.8862745 , 0.8745098 ],\n",
       "         [0.93333334, 0.99215686, 0.9647059 ],\n",
       "         ...,\n",
       "         [0.29803923, 0.36078432, 0.42352942],\n",
       "         [0.28235295, 0.34901962, 0.4117647 ],\n",
       "         [0.27450982, 0.36078432, 0.44313726]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.00392157, 0.01568628],\n",
       "         [0.        , 0.01568628, 0.01960784],\n",
       "         [0.00784314, 0.01568628, 0.01176471],\n",
       "         ...,\n",
       "         [0.2901961 , 0.21176471, 0.11372549],\n",
       "         [0.2784314 , 0.2       , 0.10196079],\n",
       "         [0.26666668, 0.1882353 , 0.09019608]],\n",
       "\n",
       "        [[0.        , 0.01568628, 0.01960784],\n",
       "         [0.        , 0.00392157, 0.        ],\n",
       "         [0.        , 0.00392157, 0.        ],\n",
       "         ...,\n",
       "         [0.2901961 , 0.21176471, 0.10588235],\n",
       "         [0.28235295, 0.20392157, 0.09803922],\n",
       "         [0.27450982, 0.19215687, 0.08627451]],\n",
       "\n",
       "        [[0.        , 0.01568628, 0.01176471],\n",
       "         [0.        , 0.00784314, 0.        ],\n",
       "         [0.03921569, 0.04313726, 0.02352941],\n",
       "         ...,\n",
       "         [0.30980393, 0.22745098, 0.11372549],\n",
       "         [0.29803923, 0.21568628, 0.10196079],\n",
       "         [0.28627452, 0.20392157, 0.09019608]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.47843137, 0.23529412, 0.14509805],\n",
       "         [0.47843137, 0.23529412, 0.14509805],\n",
       "         [0.48235294, 0.23921569, 0.14117648],\n",
       "         ...,\n",
       "         [0.28235295, 0.23529412, 0.28235295],\n",
       "         [0.2509804 , 0.20392157, 0.2509804 ],\n",
       "         [0.25882354, 0.21176471, 0.25882354]],\n",
       "\n",
       "        [[0.44705883, 0.21960784, 0.1254902 ],\n",
       "         [0.44705883, 0.21960784, 0.1254902 ],\n",
       "         [0.45490196, 0.22745098, 0.13333334],\n",
       "         ...,\n",
       "         [0.25882354, 0.21176471, 0.25882354],\n",
       "         [0.23529412, 0.1882353 , 0.23529412],\n",
       "         [0.2509804 , 0.20392157, 0.2509804 ]],\n",
       "\n",
       "        [[0.42745098, 0.20784314, 0.11764706],\n",
       "         [0.43137255, 0.21176471, 0.12156863],\n",
       "         [0.43529412, 0.21568628, 0.11764706],\n",
       "         ...,\n",
       "         [0.2627451 , 0.21568628, 0.2627451 ],\n",
       "         [0.24705882, 0.2       , 0.24705882],\n",
       "         [0.27058825, 0.22352941, 0.27058825]]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_to_tensor(train_files[:100]).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [01:24<00:00, 79.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 835/835 [00:10<00:00, 95.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:10<00:00, 79.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# Keras中的数据预处理过程\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255#贼慢 不知道是怎么回事\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.layers import Dropout,Flatten,Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               66633     \n",
      "=================================================================\n",
      "Total params: 1,900,253\n",
      "Trainable params: 1,900,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "#6个特征卷积滤波器，每个为5*5窗口对3层的图片进行处理\n",
    "model.add(Conv2D(filters=16,kernel_size=5,padding='same',activation='relu',input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))#池化层 数据量减半\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=4,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))#池化层 数据量减半\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))#池化层 数据量减半\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))#池化层 数据量减半\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Conv2D(filters=512,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))#池化层 数据量减半\n",
    "model.add(GlobalAveragePooling2D())#平均池化层将多维特征压缩为一层  每层只取一个参数\n",
    "\n",
    "model.add(Dense(500,activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(133,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 109s 16ms/step - loss: 4.8947 - acc: 0.0094 - val_loss: 4.8743 - val_acc: 0.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.87428, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 93s 14ms/step - loss: 4.8753 - acc: 0.0111 - val_loss: 4.8708 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.87428 to 4.87082, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 83s 12ms/step - loss: 4.8729 - acc: 0.0091 - val_loss: 4.8707 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.87082 to 4.87070, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 85s 13ms/step - loss: 4.8704 - acc: 0.0109 - val_loss: 4.8686 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.87070 to 4.86858, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 83s 12ms/step - loss: 4.8694 - acc: 0.0093 - val_loss: 4.8686 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.86858\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 84s 13ms/step - loss: 4.8699 - acc: 0.0105 - val_loss: 4.8686 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.86858\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 95s 14ms/step - loss: 4.8670 - acc: 0.0115 - val_loss: 4.8691 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.86858\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 90s 14ms/step - loss: 4.8676 - acc: 0.0099 - val_loss: 4.8686 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.86858\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 95s 14ms/step - loss: 4.8669 - acc: 0.0108 - val_loss: 4.8686 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.86858\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 85s 13ms/step - loss: 4.8661 - acc: 0.0100 - val_loss: 4.8687 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.86858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b9480dda0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "epochs=10\n",
    "#勿改\n",
    "checkpointer=ModelCheckpoint(filepath=r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5',verbose=1,save_best_only=True)\n",
    "model.fit(train_tensors,train_targets,\n",
    "         validation_data=(valid_tensors,valid_targets),\n",
    "         epochs=epochs,batch_size=20,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model.load_weights(r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取测试数据集中每一个图像所预测的狗的品种的index\n",
    "dog_breed_predictions=[np.argmax(model.predict(np.expand_dims(tensor,axis=0))) for tensor in test_tensors]\n",
    "dog_breed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1961722488038278"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy=100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets,axis=1))/len(dog_breed_predictions)\n",
    "test_accuracy #判断品种的准确率贼低  自己构建的网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经穿过vgg16的数据 得到从图像中提取的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 7, 7, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features=np.load(r'C:\\Users\\wlwy\\Documents\\four_task\\DogVGG16Data.npz')\n",
    "train_vgg16=bottleneck_features['train']\n",
    "valid_vgg16=bottleneck_features['valid']\n",
    "test_vgg16=bottleneck_features['test']\n",
    "np.shape(train_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vgg16.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model=Sequential()\n",
    "vgg16_model.add(GlobalAveragePooling2D(input_shape=(7,7,512)))#全局池化，按照最后一维来  压缩成1*1*512\n",
    "#Flatten()数据量过大\n",
    "vgg16_model.add(Dense(133,activation='softmax'))\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 4s 666us/step - loss: 7.1067 - acc: 0.5510 - val_loss: 8.0353 - val_acc: 0.4287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.03535, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 4s 566us/step - loss: 7.0696 - acc: 0.5527 - val_loss: 7.9619 - val_acc: 0.4383\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.03535 to 7.96186, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 4s 606us/step - loss: 6.9691 - acc: 0.5605 - val_loss: 7.9044 - val_acc: 0.4431\n",
      "\n",
      "Epoch 00003: val_loss improved from 7.96186 to 7.90440, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 4s 569us/step - loss: 6.6774 - acc: 0.5657 - val_loss: 7.6244 - val_acc: 0.4551\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.90440 to 7.62438, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 4s 579us/step - loss: 6.5023 - acc: 0.5817 - val_loss: 7.4475 - val_acc: 0.4587\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.62438 to 7.44751, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 4s 555us/step - loss: 6.4232 - acc: 0.5919 - val_loss: 7.3899 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.44751 to 7.38986, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 4s 531us/step - loss: 6.3965 - acc: 0.5984 - val_loss: 7.4163 - val_acc: 0.4719\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 7.38986\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 3s 510us/step - loss: 6.3872 - acc: 0.6009 - val_loss: 7.4221 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 7.38986\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 3s 520us/step - loss: 6.3729 - acc: 0.6000 - val_loss: 7.3687 - val_acc: 0.4826\n",
      "\n",
      "Epoch 00009: val_loss improved from 7.38986 to 7.36872, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 3s 495us/step - loss: 6.2507 - acc: 0.5997 - val_loss: 7.3376 - val_acc: 0.4683\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.36872 to 7.33757, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 3s 481us/step - loss: 6.0413 - acc: 0.6085 - val_loss: 7.1308 - val_acc: 0.4862\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.33757 to 7.13078, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 3s 460us/step - loss: 5.9150 - acc: 0.6208 - val_loss: 7.1418 - val_acc: 0.4659\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 7.13078\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 3s 459us/step - loss: 5.8220 - acc: 0.6247 - val_loss: 7.0482 - val_acc: 0.4754\n",
      "\n",
      "Epoch 00013: val_loss improved from 7.13078 to 7.04825, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 3s 446us/step - loss: 5.6373 - acc: 0.6293 - val_loss: 6.8070 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.04825 to 6.80697, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 5.4997 - acc: 0.6464 - val_loss: 6.7211 - val_acc: 0.5030\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.80697 to 6.72114, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 5s 781us/step - loss: 5.4599 - acc: 0.6518 - val_loss: 6.7672 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.72114\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 3s 381us/step - loss: 5.4278 - acc: 0.6545 - val_loss: 6.7833 - val_acc: 0.5054\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.72114\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 3s 396us/step - loss: 5.4107 - acc: 0.6579 - val_loss: 6.6825 - val_acc: 0.5102\n",
      "\n",
      "Epoch 00018: val_loss improved from 6.72114 to 6.68252, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 3s 435us/step - loss: 5.3984 - acc: 0.6599 - val_loss: 6.7411 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.68252\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 3s 414us/step - loss: 5.3904 - acc: 0.6627 - val_loss: 6.6454 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00020: val_loss improved from 6.68252 to 6.64539, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b9a516278>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer=ModelCheckpoint(filepath=r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5',verbose=1,save_best_only=True)\n",
    "vgg16_model.fit(train_vgg16,train_targets,\n",
    "                validation_data=(valid_vgg16,valid_targets),\n",
    "                epochs=20,batch_size=20,\n",
    "                callbacks=[checkpointer],\n",
    "                verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载最好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_model.load_weights(r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.vgg16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56,\n",
       " 101,\n",
       " 47,\n",
       " 18,\n",
       " 1,\n",
       " 92,\n",
       " 44,\n",
       " 51,\n",
       " 25,\n",
       " 3,\n",
       " 1,\n",
       " 18,\n",
       " 81,\n",
       " 37,\n",
       " 19,\n",
       " 80,\n",
       " 5,\n",
       " 68,\n",
       " 90,\n",
       " 106,\n",
       " 97,\n",
       " 68,\n",
       " 60,\n",
       " 94,\n",
       " 35,\n",
       " 8,\n",
       " 18,\n",
       " 18,\n",
       " 83,\n",
       " 81,\n",
       " 4,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 105,\n",
       " 27,\n",
       " 34,\n",
       " 42,\n",
       " 3,\n",
       " 44,\n",
       " 80,\n",
       " 36,\n",
       " 106,\n",
       " 26,\n",
       " 60,\n",
       " 35,\n",
       " 100,\n",
       " 89,\n",
       " 87,\n",
       " 7,\n",
       " 32,\n",
       " 47,\n",
       " 73,\n",
       " 44,\n",
       " 34,\n",
       " 118,\n",
       " 101,\n",
       " 103,\n",
       " 106,\n",
       " 86,\n",
       " 10,\n",
       " 102,\n",
       " 33,\n",
       " 95,\n",
       " 81,\n",
       " 27,\n",
       " 126,\n",
       " 95,\n",
       " 115,\n",
       " 37,\n",
       " 87,\n",
       " 57,\n",
       " 81,\n",
       " 69,\n",
       " 19,\n",
       " 45,\n",
       " 86,\n",
       " 113,\n",
       " 37,\n",
       " 54,\n",
       " 127,\n",
       " 102,\n",
       " 39,\n",
       " 81,\n",
       " 92,\n",
       " 86,\n",
       " 27,\n",
       " 45,\n",
       " 5,\n",
       " 36,\n",
       " 93,\n",
       " 45,\n",
       " 130,\n",
       " 101,\n",
       " 83,\n",
       " 19,\n",
       " 75,\n",
       " 101,\n",
       " 8,\n",
       " 83,\n",
       " 7,\n",
       " 10,\n",
       " 55,\n",
       " 23,\n",
       " 71,\n",
       " 90,\n",
       " 51,\n",
       " 61,\n",
       " 83,\n",
       " 86,\n",
       " 98,\n",
       " 45,\n",
       " 43,\n",
       " 11,\n",
       " 3,\n",
       " 10,\n",
       " 118,\n",
       " 42,\n",
       " 27,\n",
       " 49,\n",
       " 55,\n",
       " 0,\n",
       " 131,\n",
       " 98,\n",
       " 36,\n",
       " 109,\n",
       " 80,\n",
       " 78,\n",
       " 43,\n",
       " 0,\n",
       " 48,\n",
       " 83,\n",
       " 26,\n",
       " 44,\n",
       " 69,\n",
       " 1,\n",
       " 19,\n",
       " 120,\n",
       " 11,\n",
       " 49,\n",
       " 47,\n",
       " 88,\n",
       " 47,\n",
       " 55,\n",
       " 4,\n",
       " 2,\n",
       " 83,\n",
       " 43,\n",
       " 123,\n",
       " 120,\n",
       " 87,\n",
       " 100,\n",
       " 36,\n",
       " 42,\n",
       " 83,\n",
       " 1,\n",
       " 95,\n",
       " 9,\n",
       " 111,\n",
       " 16,\n",
       " 97,\n",
       " 11,\n",
       " 90,\n",
       " 87,\n",
       " 95,\n",
       " 37,\n",
       " 130,\n",
       " 97,\n",
       " 47,\n",
       " 90,\n",
       " 71,\n",
       " 120,\n",
       " 44,\n",
       " 59,\n",
       " 35,\n",
       " 72,\n",
       " 119,\n",
       " 35,\n",
       " 111,\n",
       " 47,\n",
       " 55,\n",
       " 100,\n",
       " 95,\n",
       " 121,\n",
       " 49,\n",
       " 121,\n",
       " 45,\n",
       " 115,\n",
       " 4,\n",
       " 68,\n",
       " 46,\n",
       " 47,\n",
       " 83,\n",
       " 123,\n",
       " 36,\n",
       " 45,\n",
       " 60,\n",
       " 10,\n",
       " 8,\n",
       " 97,\n",
       " 47,\n",
       " 83,\n",
       " 69,\n",
       " 77,\n",
       " 19,\n",
       " 101,\n",
       " 42,\n",
       " 4,\n",
       " 93,\n",
       " 45,\n",
       " 90,\n",
       " 121,\n",
       " 1,\n",
       " 83,\n",
       " 37,\n",
       " 52,\n",
       " 16,\n",
       " 49,\n",
       " 23,\n",
       " 52,\n",
       " 33,\n",
       " 109,\n",
       " 44,\n",
       " 102,\n",
       " 111,\n",
       " 120,\n",
       " 124,\n",
       " 19,\n",
       " 45,\n",
       " 54,\n",
       " 119,\n",
       " 36,\n",
       " 95,\n",
       " 43,\n",
       " 119,\n",
       " 32,\n",
       " 2,\n",
       " 45,\n",
       " 55,\n",
       " 104,\n",
       " 95,\n",
       " 47,\n",
       " 49,\n",
       " 56,\n",
       " 77,\n",
       " 78,\n",
       " 47,\n",
       " 33,\n",
       " 102,\n",
       " 52,\n",
       " 11,\n",
       " 25,\n",
       " 90,\n",
       " 45,\n",
       " 5,\n",
       " 88,\n",
       " 130,\n",
       " 10,\n",
       " 45,\n",
       " 26,\n",
       " 42,\n",
       " 61,\n",
       " 55,\n",
       " 126,\n",
       " 2,\n",
       " 86,\n",
       " 34,\n",
       " 33,\n",
       " 131,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 118,\n",
       " 95,\n",
       " 27,\n",
       " 95,\n",
       " 51,\n",
       " 126,\n",
       " 88,\n",
       " 44,\n",
       " 36,\n",
       " 44,\n",
       " 49,\n",
       " 83,\n",
       " 32,\n",
       " 92,\n",
       " 116,\n",
       " 111,\n",
       " 71,\n",
       " 103,\n",
       " 83,\n",
       " 57,\n",
       " 19,\n",
       " 59,\n",
       " 56,\n",
       " 34,\n",
       " 48,\n",
       " 68,\n",
       " 36,\n",
       " 83,\n",
       " 83,\n",
       " 81,\n",
       " 106,\n",
       " 46,\n",
       " 83,\n",
       " 55,\n",
       " 45,\n",
       " 92,\n",
       " 68,\n",
       " 36,\n",
       " 19,\n",
       " 43,\n",
       " 116,\n",
       " 34,\n",
       " 94,\n",
       " 16,\n",
       " 1,\n",
       " 33,\n",
       " 46,\n",
       " 0,\n",
       " 61,\n",
       " 55,\n",
       " 44,\n",
       " 81,\n",
       " 116,\n",
       " 106,\n",
       " 32,\n",
       " 55,\n",
       " 11,\n",
       " 75,\n",
       " 68,\n",
       " 54,\n",
       " 106,\n",
       " 89,\n",
       " 48,\n",
       " 11,\n",
       " 60,\n",
       " 25,\n",
       " 94,\n",
       " 98,\n",
       " 26,\n",
       " 94,\n",
       " 55,\n",
       " 47,\n",
       " 11,\n",
       " 23,\n",
       " 127,\n",
       " 43,\n",
       " 126,\n",
       " 116,\n",
       " 11,\n",
       " 73,\n",
       " 92,\n",
       " 106,\n",
       " 34,\n",
       " 47,\n",
       " 72,\n",
       " 10,\n",
       " 11,\n",
       " 101,\n",
       " 81,\n",
       " 16,\n",
       " 0,\n",
       " 4,\n",
       " 11,\n",
       " 97,\n",
       " 51,\n",
       " 121,\n",
       " 101,\n",
       " 69,\n",
       " 75,\n",
       " 30,\n",
       " 115,\n",
       " 44,\n",
       " 93,\n",
       " 42,\n",
       " 36,\n",
       " 121,\n",
       " 44,\n",
       " 78,\n",
       " 83,\n",
       " 98,\n",
       " 33,\n",
       " 81,\n",
       " 42,\n",
       " 16,\n",
       " 101,\n",
       " 86,\n",
       " 19,\n",
       " 116,\n",
       " 127,\n",
       " 30,\n",
       " 3,\n",
       " 59,\n",
       " 75,\n",
       " 69,\n",
       " 69,\n",
       " 55,\n",
       " 0,\n",
       " 10,\n",
       " 44,\n",
       " 48,\n",
       " 90,\n",
       " 55,\n",
       " 92,\n",
       " 102,\n",
       " 19,\n",
       " 56,\n",
       " 9,\n",
       " 88,\n",
       " 115,\n",
       " 75,\n",
       " 90,\n",
       " 11,\n",
       " 101,\n",
       " 45,\n",
       " 3,\n",
       " 81,\n",
       " 45,\n",
       " 45,\n",
       " 57,\n",
       " 93,\n",
       " 100,\n",
       " 87,\n",
       " 55,\n",
       " 56,\n",
       " 52,\n",
       " 81,\n",
       " 46,\n",
       " 49,\n",
       " 66,\n",
       " 90,\n",
       " 71,\n",
       " 121,\n",
       " 75,\n",
       " 111,\n",
       " 2,\n",
       " 32,\n",
       " 102,\n",
       " 16,\n",
       " 86,\n",
       " 92,\n",
       " 35,\n",
       " 25,\n",
       " 47,\n",
       " 34,\n",
       " 46,\n",
       " 45,\n",
       " 43,\n",
       " 69,\n",
       " 44,\n",
       " 0,\n",
       " 46,\n",
       " 59,\n",
       " 116,\n",
       " 80,\n",
       " 73,\n",
       " 57,\n",
       " 109,\n",
       " 106,\n",
       " 55,\n",
       " 55,\n",
       " 42,\n",
       " 48,\n",
       " 75,\n",
       " 83,\n",
       " 44,\n",
       " 4,\n",
       " 46,\n",
       " 113,\n",
       " 7,\n",
       " 16,\n",
       " 102,\n",
       " 121,\n",
       " 32,\n",
       " 42,\n",
       " 7,\n",
       " 55,\n",
       " 7,\n",
       " 100,\n",
       " 68,\n",
       " 27,\n",
       " 33,\n",
       " 83,\n",
       " 77,\n",
       " 75,\n",
       " 36,\n",
       " 66,\n",
       " 118,\n",
       " 4,\n",
       " 11,\n",
       " 83,\n",
       " 10,\n",
       " 81,\n",
       " 11,\n",
       " 4,\n",
       " 130,\n",
       " 113,\n",
       " 126,\n",
       " 43,\n",
       " 113,\n",
       " 27,\n",
       " 92,\n",
       " 33,\n",
       " 83,\n",
       " 16,\n",
       " 83,\n",
       " 45,\n",
       " 7,\n",
       " 102,\n",
       " 83,\n",
       " 119,\n",
       " 68,\n",
       " 96,\n",
       " 61,\n",
       " 45,\n",
       " 59,\n",
       " 86,\n",
       " 105,\n",
       " 37,\n",
       " 80,\n",
       " 42,\n",
       " 47,\n",
       " 81,\n",
       " 0,\n",
       " 0,\n",
       " 45,\n",
       " 78,\n",
       " 89,\n",
       " 111,\n",
       " 73,\n",
       " 47,\n",
       " 36,\n",
       " 42,\n",
       " 7,\n",
       " 52,\n",
       " 101,\n",
       " 42,\n",
       " 121,\n",
       " 97,\n",
       " 61,\n",
       " 78,\n",
       " 121,\n",
       " 92,\n",
       " 83,\n",
       " 127,\n",
       " 80,\n",
       " 118,\n",
       " 90,\n",
       " 26,\n",
       " 92,\n",
       " 55,\n",
       " 83,\n",
       " 121,\n",
       " 2,\n",
       " 2,\n",
       " 77,\n",
       " 44,\n",
       " 34,\n",
       " 19,\n",
       " 106,\n",
       " 55,\n",
       " 89,\n",
       " 86,\n",
       " 59,\n",
       " 11,\n",
       " 106,\n",
       " 94,\n",
       " 77,\n",
       " 105,\n",
       " 88,\n",
       " 111,\n",
       " 116,\n",
       " 43,\n",
       " 27,\n",
       " 48,\n",
       " 9,\n",
       " 130,\n",
       " 72,\n",
       " 46,\n",
       " 30,\n",
       " 118,\n",
       " 19,\n",
       " 39,\n",
       " 3,\n",
       " 121,\n",
       " 71,\n",
       " 94,\n",
       " 126,\n",
       " 23,\n",
       " 11,\n",
       " 47,\n",
       " 51,\n",
       " 43,\n",
       " 106,\n",
       " 106,\n",
       " 93,\n",
       " 25,\n",
       " 69,\n",
       " 61,\n",
       " 105,\n",
       " 8,\n",
       " 111,\n",
       " 39,\n",
       " 71,\n",
       " 131,\n",
       " 68,\n",
       " 4,\n",
       " 111,\n",
       " 5,\n",
       " 46,\n",
       " 7,\n",
       " 123,\n",
       " 26,\n",
       " 69,\n",
       " 90,\n",
       " 54,\n",
       " 43,\n",
       " 10,\n",
       " 54,\n",
       " 78,\n",
       " 83,\n",
       " 30,\n",
       " 116,\n",
       " 123,\n",
       " 105,\n",
       " 86,\n",
       " 55,\n",
       " 102,\n",
       " 89,\n",
       " 57,\n",
       " 10,\n",
       " 44,\n",
       " 88,\n",
       " 105,\n",
       " 104,\n",
       " 109,\n",
       " 44,\n",
       " 68,\n",
       " 9,\n",
       " 3,\n",
       " 52,\n",
       " 37,\n",
       " 90,\n",
       " 47,\n",
       " 95,\n",
       " 19,\n",
       " 11,\n",
       " 60,\n",
       " 78,\n",
       " 37,\n",
       " 106,\n",
       " 45,\n",
       " 98,\n",
       " 16,\n",
       " 39,\n",
       " 34,\n",
       " 83,\n",
       " 46,\n",
       " 121,\n",
       " 47,\n",
       " 69,\n",
       " 60,\n",
       " 121,\n",
       " 27,\n",
       " 3,\n",
       " 60,\n",
       " 19,\n",
       " 19,\n",
       " 75,\n",
       " 89,\n",
       " 45,\n",
       " 86,\n",
       " 18,\n",
       " 86,\n",
       " 44,\n",
       " 48,\n",
       " 43,\n",
       " 52,\n",
       " 56,\n",
       " 87,\n",
       " 86,\n",
       " 44,\n",
       " 102,\n",
       " 37,\n",
       " 30,\n",
       " 126,\n",
       " 95,\n",
       " 92,\n",
       " 42,\n",
       " 39,\n",
       " 45,\n",
       " 126,\n",
       " 52,\n",
       " 55,\n",
       " 66,\n",
       " 126,\n",
       " 89,\n",
       " 37,\n",
       " 119,\n",
       " 23,\n",
       " 4,\n",
       " 10,\n",
       " 106,\n",
       " 116,\n",
       " 5,\n",
       " 11,\n",
       " 4,\n",
       " 27,\n",
       " 7,\n",
       " 5,\n",
       " 56,\n",
       " 80,\n",
       " 11,\n",
       " 19,\n",
       " 60,\n",
       " 55,\n",
       " 83,\n",
       " 44,\n",
       " 51,\n",
       " 16,\n",
       " 59,\n",
       " 3,\n",
       " 106,\n",
       " 9,\n",
       " 105,\n",
       " 72,\n",
       " 52,\n",
       " 27,\n",
       " 66,\n",
       " 27,\n",
       " 88,\n",
       " 30,\n",
       " 33,\n",
       " 102,\n",
       " 35,\n",
       " 87,\n",
       " 26,\n",
       " 106,\n",
       " 33,\n",
       " 121,\n",
       " 11,\n",
       " 83,\n",
       " 106,\n",
       " 80,\n",
       " 51,\n",
       " 10,\n",
       " 30,\n",
       " 32,\n",
       " 18,\n",
       " 23,\n",
       " 16,\n",
       " 5,\n",
       " 5,\n",
       " 44,\n",
       " 27,\n",
       " 36,\n",
       " 83,\n",
       " 81,\n",
       " 88,\n",
       " 54,\n",
       " 126,\n",
       " 11,\n",
       " 59,\n",
       " 55,\n",
       " 39,\n",
       " 7,\n",
       " 43,\n",
       " 35,\n",
       " 116,\n",
       " 124,\n",
       " 5,\n",
       " 47,\n",
       " 111,\n",
       " 45,\n",
       " 37,\n",
       " 115,\n",
       " 68,\n",
       " 78,\n",
       " 121,\n",
       " 80,\n",
       " 16,\n",
       " 3,\n",
       " 26,\n",
       " 45,\n",
       " 121,\n",
       " 98,\n",
       " 19,\n",
       " 33,\n",
       " 80,\n",
       " 77,\n",
       " 77,\n",
       " 73,\n",
       " 103,\n",
       " 36,\n",
       " 39,\n",
       " 57,\n",
       " 19,\n",
       " 88,\n",
       " 123,\n",
       " 126,\n",
       " 23,\n",
       " 42,\n",
       " 16,\n",
       " 59,\n",
       " 106,\n",
       " 115,\n",
       " 121,\n",
       " 119,\n",
       " 9,\n",
       " 55,\n",
       " 81,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 94,\n",
       " 68,\n",
       " 105,\n",
       " 3,\n",
       " 54,\n",
       " 86,\n",
       " 60,\n",
       " 102,\n",
       " 86,\n",
       " 9,\n",
       " 90,\n",
       " 77,\n",
       " 109,\n",
       " 1,\n",
       " 49,\n",
       " 105,\n",
       " 116,\n",
       " 26,\n",
       " 90,\n",
       " 80,\n",
       " 101]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试集中狗的品种种类\n",
    "vgg16_predictions=[np.argmax(vgg16_model.predict(np.expand_dims(feature,axis=0))) for feature in test_vgg16]\n",
    "vgg16_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.9904%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.99043062200957"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy=100*np.sum(np.array(vgg16_predictions)==np.argmax(test_targets,axis=1))/len(vgg16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合成一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from extract_bottleneck_features import *  ????\n",
    "def vgg16_predict_breed(img_path):\n",
    "    # 提取bottleneck特征\n",
    "    bottleneck_feature = extract_vgg16(path_to_tensor(img_path))\n",
    "    # 获取预测向量\n",
    "    predicted_vector = vgg16_model.predict(bottleneck_feature)\n",
    "    # 返回此模型预测的狗的品种\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features=np.load(r'C:\\Users\\wlwy\\Documents\\four_task\\DogResnet50Data.npz')\n",
    "train_ResNet=bottleneck_features['train']\n",
    "valid_ResNet=bottleneck_features['valid']\n",
    "test_ResNet=bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 1, 1, 2048)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model=Sequential()\n",
    "resnet_model.add(GlobalAveragePooling2D(input_shape=train_ResNet.shape[1:]))\n",
    "resnet_model.add(Dense(133,activation='softmax'))\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 5s 786us/step - loss: 1.6334 - acc: 0.5933 - val_loss: 0.8856 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88558, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.ResNet.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 3s 472us/step - loss: 0.4294 - acc: 0.8644 - val_loss: 0.7333 - val_acc: 0.7677\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.88558 to 0.73335, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.ResNet.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 4s 531us/step - loss: 0.2644 - acc: 0.9139 - val_loss: 0.6376 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.73335 to 0.63756, saving model to C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.ResNet.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 3s 467us/step - loss: 0.1702 - acc: 0.9478 - val_loss: 0.6378 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63756\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 3s 465us/step - loss: 0.1221 - acc: 0.9618 - val_loss: 0.6690 - val_acc: 0.7976\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63756\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 3s 505us/step - loss: 0.0883 - acc: 0.9713 - val_loss: 0.6631 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63756\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 3s 467us/step - loss: 0.0621 - acc: 0.9825 - val_loss: 0.6635 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63756\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 3s 497us/step - loss: 0.0484 - acc: 0.9864 - val_loss: 0.6829 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63756\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 5s 691us/step - loss: 0.0362 - acc: 0.9897 - val_loss: 0.7260 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63756\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 3s 429us/step - loss: 0.0295 - acc: 0.9918 - val_loss: 0.8188 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63756\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 3s 409us/step - loss: 0.0210 - acc: 0.9960 - val_loss: 0.8408 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.63756\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 3s 425us/step - loss: 0.0192 - acc: 0.9951 - val_loss: 0.8069 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.63756\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 3s 419us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.8426 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.63756\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 3s 411us/step - loss: 0.0141 - acc: 0.9975 - val_loss: 0.8433 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.63756\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 3s 402us/step - loss: 0.0101 - acc: 0.9984 - val_loss: 0.8001 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.63756\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 3s 384us/step - loss: 0.0100 - acc: 0.9981 - val_loss: 0.8491 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.63756\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 3s 378us/step - loss: 0.0089 - acc: 0.9978 - val_loss: 0.8264 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.63756\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 3s 409us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.8442 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.63756\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 3s 376us/step - loss: 0.0071 - acc: 0.9984 - val_loss: 0.8569 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.63756\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 2s 374us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.9209 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.63756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b9a614668>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "checkpointer=ModelCheckpoint(filepath=r'C:\\Users\\wlwy\\Documents\\four_task\\dogImages\\weights.best.ResNet.hdf5',\n",
    "                            verbose=1,save_best_only=True)\n",
    "resnet_model.fit(train_ResNet,train_targets,\n",
    "                validation_data=(valid_ResNet,valid_targets),\n",
    "                epochs=20,batch_size=20,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.9809%\n"
     ]
    }
   ],
   "source": [
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "ResNet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(test_ResNet)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后续的那两个操作一致"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
