{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('anna.txt','r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建字符集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=set(text)#无重复集合\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字符-数字映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 15,\n",
       " ' ': 0,\n",
       " '!': 40,\n",
       " '\"': 33,\n",
       " '$': 46,\n",
       " '%': 76,\n",
       " '&': 79,\n",
       " \"'\": 59,\n",
       " '(': 3,\n",
       " ')': 35,\n",
       " '*': 27,\n",
       " ',': 64,\n",
       " '-': 18,\n",
       " '.': 71,\n",
       " '/': 7,\n",
       " '0': 16,\n",
       " '1': 38,\n",
       " '2': 75,\n",
       " '3': 6,\n",
       " '4': 25,\n",
       " '5': 5,\n",
       " '6': 43,\n",
       " '7': 14,\n",
       " '8': 82,\n",
       " '9': 68,\n",
       " ':': 48,\n",
       " ';': 80,\n",
       " '?': 21,\n",
       " '@': 51,\n",
       " 'A': 74,\n",
       " 'B': 4,\n",
       " 'C': 70,\n",
       " 'D': 57,\n",
       " 'E': 12,\n",
       " 'F': 37,\n",
       " 'G': 56,\n",
       " 'H': 62,\n",
       " 'I': 49,\n",
       " 'J': 11,\n",
       " 'K': 29,\n",
       " 'L': 34,\n",
       " 'M': 45,\n",
       " 'N': 78,\n",
       " 'O': 28,\n",
       " 'P': 65,\n",
       " 'Q': 30,\n",
       " 'R': 55,\n",
       " 'S': 22,\n",
       " 'T': 1,\n",
       " 'U': 67,\n",
       " 'V': 26,\n",
       " 'W': 53,\n",
       " 'X': 32,\n",
       " 'Y': 66,\n",
       " 'Z': 41,\n",
       " '_': 44,\n",
       " '`': 42,\n",
       " 'a': 54,\n",
       " 'b': 69,\n",
       " 'c': 9,\n",
       " 'd': 63,\n",
       " 'e': 24,\n",
       " 'f': 77,\n",
       " 'g': 8,\n",
       " 'h': 81,\n",
       " 'i': 39,\n",
       " 'j': 58,\n",
       " 'k': 60,\n",
       " 'l': 31,\n",
       " 'm': 17,\n",
       " 'n': 52,\n",
       " 'o': 73,\n",
       " 'p': 50,\n",
       " 'q': 2,\n",
       " 'r': 72,\n",
       " 's': 19,\n",
       " 't': 61,\n",
       " 'u': 47,\n",
       " 'v': 36,\n",
       " 'w': 23,\n",
       " 'x': 10,\n",
       " 'y': 20,\n",
       " 'z': 13}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int={c:i for i ,c in enumerate(vocab)}\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数字-字符映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: 'T',\n",
       " 2: 'q',\n",
       " 3: '(',\n",
       " 4: 'B',\n",
       " 5: '5',\n",
       " 6: '3',\n",
       " 7: '/',\n",
       " 8: 'g',\n",
       " 9: 'c',\n",
       " 10: 'x',\n",
       " 11: 'J',\n",
       " 12: 'E',\n",
       " 13: 'z',\n",
       " 14: '7',\n",
       " 15: '\\n',\n",
       " 16: '0',\n",
       " 17: 'm',\n",
       " 18: '-',\n",
       " 19: 's',\n",
       " 20: 'y',\n",
       " 21: '?',\n",
       " 22: 'S',\n",
       " 23: 'w',\n",
       " 24: 'e',\n",
       " 25: '4',\n",
       " 26: 'V',\n",
       " 27: '*',\n",
       " 28: 'O',\n",
       " 29: 'K',\n",
       " 30: 'Q',\n",
       " 31: 'l',\n",
       " 32: 'X',\n",
       " 33: '\"',\n",
       " 34: 'L',\n",
       " 35: ')',\n",
       " 36: 'v',\n",
       " 37: 'F',\n",
       " 38: '1',\n",
       " 39: 'i',\n",
       " 40: '!',\n",
       " 41: 'Z',\n",
       " 42: '`',\n",
       " 43: '6',\n",
       " 44: '_',\n",
       " 45: 'M',\n",
       " 46: '$',\n",
       " 47: 'u',\n",
       " 48: ':',\n",
       " 49: 'I',\n",
       " 50: 'p',\n",
       " 51: '@',\n",
       " 52: 'n',\n",
       " 53: 'W',\n",
       " 54: 'a',\n",
       " 55: 'R',\n",
       " 56: 'G',\n",
       " 57: 'D',\n",
       " 58: 'j',\n",
       " 59: \"'\",\n",
       " 60: 'k',\n",
       " 61: 't',\n",
       " 62: 'H',\n",
       " 63: 'd',\n",
       " 64: ',',\n",
       " 65: 'P',\n",
       " 66: 'Y',\n",
       " 67: 'U',\n",
       " 68: '9',\n",
       " 69: 'b',\n",
       " 70: 'C',\n",
       " 71: '.',\n",
       " 72: 'r',\n",
       " 73: 'o',\n",
       " 74: 'A',\n",
       " 75: '2',\n",
       " 76: '%',\n",
       " 77: 'f',\n",
       " 78: 'N',\n",
       " 79: '&',\n",
       " 80: ';',\n",
       " 81: 'h',\n",
       " 82: '8'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab=dict(enumerate(vocab))\n",
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本进行转码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([70, 81, 54, ..., 19, 71, 15])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded=np.array([vocab_to_int[c] for c in text],dtype=np.int32)\n",
    "print(len(encoded))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr,n_seqs,n_steps):#10 50\n",
    "    #对数据进行分割\n",
    "    #arr :待分割的数组\n",
    "    #n_seqs:batch中序列个数 就是前多少步作为历史输入\n",
    "    #n_steps:序列长度\n",
    "    batch_size=n_seqs*n_steps#N*M 10*50\n",
    "    n_batches=int(len(arr)/batch_size)#1985223/50=39704\n",
    "    #保留可以被分割的部分 舍弃不可分割的部分\n",
    "    arr=arr[:batch_size*n_batches]# 取整数个数\n",
    "    #重塑\n",
    "    arr=arr.reshape((n_seqs,-1))#变为二维数组   [n_seqs,None]  10*198520\n",
    "    for n in range(0,arr.shape[1],n_steps):#(0,198520,50)\n",
    "        x=arr[:,n:n+n_steps]#x是10*50的二维数组\n",
    "        y=np.zeros_like(x)\n",
    "        #y比x会向后错位一个字符\n",
    "        y[:,:-1],y[:,-1]=x[:,1:],y[:,0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= (10, 50)\n",
      "y= (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches=get_batches(encoded,10,50)\n",
    "x,y=next(batches)\n",
    "print('x=',x.shape)\n",
    "print('y=',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs,num_steps):#n*m的矩阵\n",
    "    #num_seqs:每个batch中的序列个数  步长\n",
    "    #num_steps:每个序列包含的字符数  数据长度\n",
    "    inputs=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='inputs')\n",
    "    targets=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='targets')\n",
    "    keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return inputs,targets,keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.LSTM层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "    #lstm_size:隐藏节点数\n",
    "    #num_layers:lstm层的数目\n",
    "    #batch_size:num_seqs * num_steps\n",
    "    #keep_prob\n",
    "    #构建lstm的基本单元\n",
    "    #lstm=tf.nn.run_cell.BasicLSTMCell(lstm_size)#找不到该函数\n",
    "    #drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    def build_cell(lstm_size,keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    #堆叠多层\n",
    "    #cell=tf.contrib.rnn.MultiRNNCell([drop for _ in range(num_layers)])\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    return cell ,initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.输出层   \n",
    "每个字符经过LSTM后的输出大小是1xL(隐藏层的节点数量) N x M x L\n",
    "输出要与全连接层建立连接\n",
    "重塑为(N x M) x L  \n",
    "整个LSTM层到softmax层的大小为L x Vocab_size  就是字符合集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output,in_size,out_size):\n",
    "    #输出结果按照列concate,例如[[1,2,3],[7,8,9]],\n",
    "    #tf.concat的结果是[1,2,3,7,8,9]\n",
    "    #np.shape((lstm_output)=（64,50,128）\n",
    "    seq_output=tf.concat(lstm_output,1)\n",
    "    #重塑\n",
    "    x=tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    #连接LSTM输入到softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w=tf.Variable(tf.truncated_normal([in_size,out_size],stddev=0.1))\n",
    "        softmax_b=tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    logits=tf.matmul(x,softmax_w)+softmax_b\n",
    "    #概率\n",
    "    out=tf.nn.softmax(logits,name='predictions')\n",
    "    return out,logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4.训练计算误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits,targets,lstm_size,num_classes):\n",
    "    #logits  全连接层输出结果 没有经过softmax\n",
    "    #targets 目标字符\n",
    "    #lstm_size  cell隐藏层节点数\n",
    "    #num_classes vocab_size\n",
    "    \n",
    "    #对target进行编码\n",
    "    y_one_hot=tf.one_hot(targets,num_classes)\n",
    "    y_reshaped=tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    loss=tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped)\n",
    "    loss=tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.optimizer\n",
    "lstm 解决梯度弥散问题  即累加\n",
    "gradient clippling 的方式来防止梯度爆炸。\n",
    "即通过设置一个阈值，当 gradients 超过这个阈值时，就将它重置为阈值大小，这就保证了梯度不会变得很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss,learning_rate,grad_clip):\n",
    "    tvars=tf.trainable_variables()\n",
    "    #tf.clip_by_global_norm会返回 clip 以后的 gradients 以及 global_norm\n",
    "    grads,_=tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)\n",
    "    train_op=tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer=train_op.apply_gradients(zip(grads,tvars))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.模型组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class charRNN:\n",
    "    def __init__(self,num_classes,batch_size=64,num_steps=50,\n",
    "                      lstm_size=128,num_layers=2,learning_rate=0.001,\n",
    "                      grad_clip=5,sampling=False):\n",
    "        #采用SGD\n",
    "        if sampling==True:\n",
    "            batch_size,num_steps=1,1\n",
    "        else:\n",
    "            batch_size,num_steps=batch_size,num_steps\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #输入\n",
    "        self.inputs,self.targets,self.keep_prob=build_inputs(batch_size,num_steps)\n",
    "        \n",
    "        #lstm\n",
    "        cell,self.initial_state=build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        \n",
    "        #编码\n",
    "        x_one_hot=tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        #运行RNN\n",
    "        outputs,state=tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state=state\n",
    "        \n",
    "        #预测结果\n",
    "        self.prediction,self.logits=build_output(outputs,lstm_size,num_classes)\n",
    "        \n",
    "        #loss 和 optimizer\n",
    "        self.loss=build_loss(self.logits,self.targets,lstm_size,num_classes)\n",
    "        self.optimizer=build_optimizer(self.loss,learning_rate,grad_clip)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "num_steps=100\n",
    "lstm_size=512\n",
    "num_layers=2\n",
    "learning_rate=0.001\n",
    "keep_prob=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-3ca99b3b7679>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-166ee5157639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                                              \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                              model.optimizer],\n\u001b[0;32m---> 27\u001b[0;31m                                              feed_dict=feed)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "#每n轮进行一次变量保存\n",
    "save_every_n=200\n",
    "\n",
    "model=charRNN(len(vocab),batch_size=batch_size,num_steps=num_steps,\n",
    "              lstm_size=lstm_size,num_layers=num_layers,\n",
    "              learning_rate=learning_rate)\n",
    "\n",
    "saver=tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    count=0\n",
    "    for e in range(epochs):\n",
    "        new_state=sess.run(model.initial_state)\n",
    "        loss=0\n",
    "        for x,y in get_batches(encoded,batch_size,num_steps):\n",
    "            count+=1\n",
    "            start=time.time()\n",
    "            feed={model.inputs:x,\n",
    "                  model.targets:y,\n",
    "                  model.keep_prob:keep_prob,\n",
    "                  model.initial_state:new_state}\n",
    "            \n",
    "            batch_loss,new_state,_=sess.run([model.loss,\n",
    "                                             model.final_state,\n",
    "                                             model.optimizer],\n",
    "                                             feed_dict=feed)\n",
    "            \n",
    "            end=time.time()\n",
    "            if count%10==0:\n",
    "                print('轮数：{}/{}...'.format(e+1,epochs),\n",
    "                      '训练步数：{}...'.format(count),\n",
    "                      '训练误差：{:.4f}...'.format(batch_loss),\n",
    "                      '{:,.4f} sec/batch'.format((end-start)))\n",
    "            if (count % save_every_n==0):\n",
    "                saver.save(sess,'checkpoints/i{}.ckpt'.format(count,lstm_size))\n",
    "    saver.save(sess,'checkpoints/i{}.ckpt'.format(count,lstm_size))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
