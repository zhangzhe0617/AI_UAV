{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer#Tokenizer是一个分词器的模块\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入已经帮助我们很方便地进行预处理。 每个评论被编码为一系列索引，对应于评论中的单词。 词按频率排序，所以整数 1 对应于最频繁的词（“the”），整数 2 对应于第二频繁的词。按照惯例，整数0对应于未知词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 然后，你通过简单地连接这些整数，将句子变成一个向量。 比如，如果句子是“是或不是”。 这些词的索引如下：\n",
    "\n",
    "### \"to\": 5\n",
    "### \"be\": 8\n",
    "### \"or\": 21\n",
    "### \"not\": 3\n",
    "### 那么，这个句子就会被编码为矢量[5,8,21,3,5,8]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment.\n",
    "该数据集预先加载了 Keras，所以一个简单的命令就会帮助我们训练和测试数据。 这里有一个我们想看多少单词的参数。 我们已将它设置为1000，但你可以随时尝试设置为其他数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)#num_words代表常用词 1000个词\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative.\n",
    "请注意，数据已经过预处理，其中所有单词都包含数字，评论作为向量与评论中包含的单词一起出现。 例如，如果单词'the'是我们词典中的第一个单词，并且评论包含单词'the'，那么在相应的向量中有 1。\n",
    "\n",
    "输出结果是 1 和 0 的向量，其中 1 表示正面评论，0 是负面评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train[1])) #1\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 999\n"
     ]
    }
   ],
   "source": [
    "x_train #这里包含的是单词表的索引值，即一句评论对应的每个单词的索引\n",
    "l_max=[max(x) for x in x_train ]#最大索引1000\n",
    "print(l_max[0],max(l_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "l_min=[min(x) for x in x_train ]#最小索引1\n",
    "print(l_min[0],min(l_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1.\n",
    "在这里，我们将输入向量转换为 (0,1)-向量。 例如，如果预处理的向量包含数字 14，则在处理的向量中，第 14 个输入将是 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "#Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类。\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print(x_train[0])#例如上面973索引对应第973位的编码为1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train[2]),np.shape(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                610       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 106,792\n",
      "Trainable params: 106,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\zhangwenqi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\zhangwenqi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()    #网络结构是1000*512*60*2\n",
    "model.add(Dense(100, activation='sigmoid', input_dim=1000))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(60,activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 0.7072 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 2/40\n",
      " - 2s - loss: 0.6953 - acc: 0.5068 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 3/40\n",
      " - 2s - loss: 0.6933 - acc: 0.5095 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/40\n",
      " - 2s - loss: 0.6928 - acc: 0.5073 - val_loss: 0.6930 - val_acc: 0.5234\n",
      "Epoch 5/40\n",
      " - 2s - loss: 0.6925 - acc: 0.5161 - val_loss: 0.6918 - val_acc: 0.6189\n",
      "Epoch 6/40\n",
      " - 2s - loss: 0.6904 - acc: 0.5307 - val_loss: 0.6865 - val_acc: 0.7434\n",
      "Epoch 7/40\n",
      " - 2s - loss: 0.6743 - acc: 0.5706 - val_loss: 0.6117 - val_acc: 0.7926\n",
      "Epoch 8/40\n",
      " - 2s - loss: 0.6075 - acc: 0.6644 - val_loss: 0.4991 - val_acc: 0.8155\n",
      "Epoch 9/40\n",
      " - 2s - loss: 0.5469 - acc: 0.7298 - val_loss: 0.4373 - val_acc: 0.8300\n",
      "Epoch 10/40\n",
      " - 2s - loss: 0.5052 - acc: 0.7637 - val_loss: 0.3985 - val_acc: 0.8369\n",
      "Epoch 11/40\n",
      " - 2s - loss: 0.4848 - acc: 0.7809 - val_loss: 0.3829 - val_acc: 0.8435\n",
      "Epoch 12/40\n",
      " - 2s - loss: 0.4628 - acc: 0.7943 - val_loss: 0.3667 - val_acc: 0.8490\n",
      "Epoch 13/40\n",
      " - 2s - loss: 0.4508 - acc: 0.8002 - val_loss: 0.3594 - val_acc: 0.8510\n",
      "Epoch 14/40\n",
      " - 2s - loss: 0.4369 - acc: 0.8080 - val_loss: 0.3489 - val_acc: 0.8538\n",
      "Epoch 15/40\n",
      " - 2s - loss: 0.4228 - acc: 0.8243 - val_loss: 0.3433 - val_acc: 0.8534\n",
      "Epoch 16/40\n",
      " - 2s - loss: 0.4145 - acc: 0.8282 - val_loss: 0.3395 - val_acc: 0.8567\n",
      "Epoch 17/40\n",
      " - 2s - loss: 0.4050 - acc: 0.8330 - val_loss: 0.3370 - val_acc: 0.8579\n",
      "Epoch 18/40\n",
      " - 2s - loss: 0.4038 - acc: 0.8346 - val_loss: 0.3330 - val_acc: 0.8582\n",
      "Epoch 19/40\n",
      " - 2s - loss: 0.3970 - acc: 0.8361 - val_loss: 0.3313 - val_acc: 0.8599\n",
      "Epoch 20/40\n",
      " - 2s - loss: 0.3927 - acc: 0.8404 - val_loss: 0.3295 - val_acc: 0.8599\n",
      "Epoch 21/40\n",
      " - 2s - loss: 0.3878 - acc: 0.8402 - val_loss: 0.3295 - val_acc: 0.8592\n",
      "Epoch 22/40\n",
      " - 2s - loss: 0.3850 - acc: 0.8423 - val_loss: 0.3261 - val_acc: 0.8611\n",
      "Epoch 23/40\n",
      " - 2s - loss: 0.3776 - acc: 0.8430 - val_loss: 0.3243 - val_acc: 0.8614\n",
      "Epoch 24/40\n",
      " - 2s - loss: 0.3757 - acc: 0.8463 - val_loss: 0.3241 - val_acc: 0.8625\n",
      "Epoch 25/40\n",
      " - 2s - loss: 0.3766 - acc: 0.8454 - val_loss: 0.3231 - val_acc: 0.8629\n",
      "Epoch 26/40\n",
      " - 2s - loss: 0.3768 - acc: 0.8451 - val_loss: 0.3233 - val_acc: 0.8640\n",
      "Epoch 27/40\n",
      " - 2s - loss: 0.3734 - acc: 0.8487 - val_loss: 0.3223 - val_acc: 0.8642\n",
      "Epoch 28/40\n",
      " - 2s - loss: 0.3720 - acc: 0.8486 - val_loss: 0.3224 - val_acc: 0.8639\n",
      "Epoch 29/40\n",
      " - 2s - loss: 0.3688 - acc: 0.8482 - val_loss: 0.3208 - val_acc: 0.8637\n",
      "Epoch 30/40\n",
      " - 2s - loss: 0.3674 - acc: 0.8532 - val_loss: 0.3208 - val_acc: 0.8625\n",
      "Epoch 31/40\n",
      " - 2s - loss: 0.3591 - acc: 0.8543 - val_loss: 0.3199 - val_acc: 0.8626\n",
      "Epoch 32/40\n",
      " - 2s - loss: 0.3676 - acc: 0.8478 - val_loss: 0.3209 - val_acc: 0.8637\n",
      "Epoch 33/40\n",
      " - 2s - loss: 0.3590 - acc: 0.8535 - val_loss: 0.3218 - val_acc: 0.8634\n",
      "Epoch 34/40\n",
      " - 2s - loss: 0.3602 - acc: 0.8506 - val_loss: 0.3207 - val_acc: 0.8624\n",
      "Epoch 35/40\n",
      " - 2s - loss: 0.3624 - acc: 0.8544 - val_loss: 0.3200 - val_acc: 0.8621\n",
      "Epoch 36/40\n",
      " - 2s - loss: 0.3588 - acc: 0.8531 - val_loss: 0.3209 - val_acc: 0.8612\n",
      "Epoch 37/40\n",
      " - 2s - loss: 0.3567 - acc: 0.8528 - val_loss: 0.3206 - val_acc: 0.8632\n",
      "Epoch 38/40\n",
      " - 2s - loss: 0.3549 - acc: 0.8557 - val_loss: 0.3207 - val_acc: 0.8627\n",
      "Epoch 39/40\n",
      " - 2s - loss: 0.3555 - acc: 0.8545 - val_loss: 0.3198 - val_acc: 0.8624\n",
      "Epoch 40/40\n",
      " - 2s - loss: 0.3536 - acc: 0.8537 - val_loss: 0.3198 - val_acc: 0.8622\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=40,\n",
    "          validation_data=(x_test, y_test), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8622\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
