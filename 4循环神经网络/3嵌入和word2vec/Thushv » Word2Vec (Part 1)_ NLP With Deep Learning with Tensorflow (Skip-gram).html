<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0115)http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/ -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head profile="http://gmpg.org/xfn/11"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Thushv  » Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)</title>

<link rel="stylesheet" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/style.css" type="text/css" media="screen,projection">

<!-- Required meta tags -->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<!-- Bootstrap CSS -->

<!--link rel="stylesheet" href="" crossorigin="anonymous"-->
<!-- Google web fonts -->
<link rel="stylesheet" type="text/css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/css">
<link rel="stylesheet" type="text/css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/css(1)">
<link rel="stylesheet" type="text/css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/css(2)">

<!-- Font awesome Icons -->
<link href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/font-awesome.min.css" rel="stylesheet">

<!--Prints head info on the front end -->
<link rel="dns-prefetch" href="http://s0.wp.com/">
<link rel="dns-prefetch" href="http://s.gravatar.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
<link rel="alternate" type="application/rss+xml" title="Thushv » Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram) Comments Feed" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/feed/">
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.thushv.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.9.8"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/wp-emoji-release.min.js.下载" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="wp-quicklatex-format-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/quicklatex-format.css" type="text/css" media="all">
<link rel="stylesheet" id="bootstrap-css-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/bootstrap.min.css" type="text/css" media="all">
<link rel="stylesheet" id="jetpack-widget-social-icons-styles-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/social-icons.css" type="text/css" media="all">
<link rel="stylesheet" id="slb_core-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/app.css" type="text/css" media="all">
<link rel="stylesheet" id="jetpack_css-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/jetpack.css" type="text/css" media="all">
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/jquery.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/jquery-migrate.min.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/wp-quicklatex-frontend.js.下载"></script>
<link rel="https://api.w.org/" href="http://www.thushv.com/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.thushv.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.thushv.com/wp-includes/wlwmanifest.xml"> 
<link rel="prev" title="Walkthrough: Working with Tensorflow on Windows through Docker" href="http://www.thushv.com/tensorflow/a-guide-to-working-with-tensorflow-on-windows-through-docker/">
<link rel="next" title="Word2Vec (Part 2): NLP With Deep Learning with Tensorflow (CBOW)" href="http://www.thushv.com/natural_language_processing/word2vec-part-2-nlp-with-deep-learning-with-tensorflow-cbow/">
<meta name="generator" content="WordPress 4.9.8">
<link rel="canonical" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/">
<link rel="shortlink" href="https://wp.me/p82pYa-2Q">
<link rel="alternate" type="application/json+oembed" href="http://www.thushv.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F">
<link rel="alternate" type="text/xml+oembed" href="http://www.thushv.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F&amp;format=xml">

<link rel="dns-prefetch" href="http://v0.wordpress.com/">
<link rel="dns-prefetch" href="http://i0.wp.com/">
<link rel="dns-prefetch" href="http://i1.wp.com/">
<link rel="dns-prefetch" href="http://i2.wp.com/">
<style type="text/css">img#wpstats{display:none}</style>
<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article">
<meta property="og:title" content="Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)">
<meta property="og:url" content="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/">
<meta property="og:description" content="G’day, I will be writing about 2 popular techniques for converting words to vectors; Skip-gram model and Continuous Bag of Words (CBOW). These are unsupervised learning methods to learn the c…">
<meta property="article:published_time" content="2016-12-03T00:32:35+00:00">
<meta property="article:modified_time" content="2017-12-24T21:09:14+00:00">
<meta property="og:site_name" content="Thushv">
<meta property="og:image" content="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=512%2C512">
<meta property="og:image:width" content="512">
<meta property="og:image:height" content="512">
<meta name="twitter:text:title" content="Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)">
<meta name="twitter:image" content="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=240%2C240">
<meta name="twitter:card" content="summary">
<link rel="stylesheet" type="text/css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/shCore.css"><link rel="stylesheet" type="text/css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/shThemeDefault.css"><style type="text/css" id="syntaxhighlighteranchor"></style>
<link rel="icon" href="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=32%2C32" sizes="32x32">
<link rel="icon" href="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=192%2C192" sizes="192x192">
<link rel="apple-touch-icon-precomposed" href="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=180%2C180">
<meta name="msapplication-TileImage" content="https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/04/cropped-favicon.png?fit=270%2C270">
 
<script async="" type="text/javascript" src="https://www-thushv-com.disqus.com/count.js"></script><script type="text/javascript" async="" src="https://www-thushv-com.disqus.com/embed.js"></script><link rel="stylesheet" type="text/css" id="gravatar-card-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/hovercard.min.css"><link rel="stylesheet" type="text/css" id="gravatar-card-services-css" href="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/services.min.css"></head>

<body>
	
	<div id="container" class="container-fluid">	
		<div id="container-header" class="row">
			<nav class="navbar navbar-expand-md navbar-dark"> 
				<!-- For mobile devices toggle button appears showing menu items --> 
				<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				<a class="navbar-brand" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#"><span id="brand-icon"><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/logo.png" width="35px" height="35px" alt=""></span></a> 
				
				<!-- Collect the nav links, forms, and other content for toggling --> 
				<div class="collapse navbar-collapse" id="navbarNavDropdown"> 
					<ul id="menu-primary" class="navbar-nav"><li id="menu-item-424" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home nav-item menu-item-424"><a title="Home" class="nav-link" href="http://www.thushv.com/">Home</a></li>
<li id="menu-item-425" class="menu-item menu-item-type-post_type menu-item-object-page nav-item menu-item-425"><a title="About" class="nav-link" href="http://www.thushv.com/about/">About</a></li>
<li id="menu-item-637" class="menu-item menu-item-type-post_type menu-item-object-page nav-item menu-item-637"><a title="My Publications" class="nav-link" href="http://www.thushv.com/my-publications/">My Publications</a></li>
<li id="menu-item-821" class="menu-item menu-item-type-post_type menu-item-object-page nav-item menu-item-821"><a title="Online CV" class="nav-link" href="http://www.thushv.com/online-cv/">Online CV</a></li>
</ul>				</div>
			</nav>
		</div>


<div class="single-container-outer row">
			
		<div class="single-container-sidebar bg-light col-md-3 d-none d-md-block pt-4">		
			<nav class="navbar  flex-column post-topics-nav" role="navigation">	
				
				<!-- Collect the nav links, forms, and other content for toggling --> 
				<h5 class="card-header">Table of Content</h5>

				<ul class="nav nav-pills flex-column">
					<li class="nav-item nav-h2"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#introduction-to">Introduction to Word2Vec</a></li><li class="nav-item nav-h3"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#embeddings">Embeddings</a></li><li class="nav-item nav-h2"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#skip-gram-model;">Skip-gram Model; an Approach to Learning Embeddings</a></li><li class="nav-item nav-h3"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#overview">Overview</a></li><li class="nav-item nav-h3"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#intuition">Intuition</a></li><li class="nav-item nav-h3"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#the-model">The model</a></li><li class="nav-item nav-h3"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#skip-gram-model">Skip-gram Model</a></li><li class="nav-item nav-h4"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#intuition-(data">Intuition (Data Generation)</a></li><li class="nav-item nav-h4"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#-training"> Training the Model </a></li><li class="nav-item nav-h4"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#results">Results</a></li><li class="nav-item nav-h2"> <a class="nav-link" href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#new-word2vec">New Word2Vec technique: GloVe</a></li>	
				</ul>

			</nav>
		</div>
	
	
		<div class="single-container-main col-md-9 col-sm-12 col-12">
			<div class="row">
			<div class="col-10">
			<div class="single-container-main-inner p-4">
								<div class="single-container-title">
					<h2>Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)</h2>
					<p class="blog-post-meta lead">Published on <a href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/#"> December 3, 2016</a></p>
				</div>
				<hr>				
				<div class="single-container-content">
					<p>G’day,</p>
<p>I will be writing about 2 popular techniques for converting words to vectors; Skip-gram model and Continuous Bag of Words (CBOW). These are unsupervised learning methods to learn the context of words. This post is structured as follows. First I’ll talk about the motivation for Word2Vec techniques. Next I will delve into the details about how  Skip-gram and CBOW work. Next I will explain the algorithms in a different vantage point (i.e. implementation). Finally, I will point out technical concerns that might help you to code the algorithms. This will be explained inline with assignment <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb" target="_blank">5_word2vec.ipynb</a> from Udacity course Deep Learning and python syntax if coding required.</p>
<h2>Introduction to Word2Vec</h2>
<p>Word2Vec is motivated as an effective technique to elicit knowledge from large text corpora in an unsupervised manner. Given that Web (or even just Wikipedia) holds copious amounts of text, it would be immensely beneficial for Natural Language Processing (NLP) to use this already available data in an unsupervised manner. It should also be understood that, labeling data is a very “tedious” and “laborious” task requiring massive number of human hours. For further reading on these techniques, refer “<a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>” by Mikolov et.al from ICML 2013 and”<a href="https://arxiv.org/abs/1310.4546" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>” by Mikolov et.al from <a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-26-2013" target="_blank">NIPS 2013</a>. Finally, this <a href="http://mccormickml.com/2016/04/27/word2vec-resources/" target="_blank">blog</a> is quite well written and helped me to understand about these topics.</p>
<h3>Embeddings</h3>
<p>So how exactly does Word2Vec help us to do NLP? This is by learning a numerical <strong>embeddings space</strong> of a vocabluary which would result in similar words ending up close to each other. For example, cat will be placed close to <code>kitten</code>, where <code>dog</code> will be placed further than <code>kitten</code> and <code>iphone</code> even further. By learning such numerical representation of the words in the vocabulary, further enables us to do various vectorized operations producing interesting results. For example, operation <code>kitten - cat + dog</code> should result in an embedded vector very close to <code>puppy</code>.</p>
<p>Below is an example of a learnt embedding space visualized using T-SNE. You can see that similar words are placed closed to each other.<br>
<img embeddings="" visualized="" with="" t-sne=""><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/embed_space.png" alt="Embedding Space" data-recalc-dims="1" class="jetpack-lazy-image--handled" data-lazy-loaded="1"><noscript><img src="https://i0.wp.com/www.thushv.com/wp-content/uploads/2016/12/embed_space.png" alt="Embedding Space" data-recalc-dims="1" /></noscript></p>
<h2>Skip-gram Model; an Approach to Learning Embeddings</h2>
<h3>Overview</h3>
<p>So we approach a step closer to the final battle; Understanding how Word2Vec works. The main idea behind skip-gram is training a model on the context of each word, so similar words will have similar numerical representations (i.e similar embedding vectors). For example, when we see the following sentence, <code>cat climbed the tree</code>, we say to the model, if you see the word <code>cat</code>, you should expect words <code>climbed </code>or <code>tree</code> very soon. If we repeat this adequately, our model will learn a representation (i.e. Embedding) of the given vocabulary.</p>
<h3>Intuition</h3>
<p>Now let’s understand how learning context of words help us to learn good embeddings. When repeating this process for 2 sentences, for example <code>cat climbed a tree</code> and <code>kitten climbed a tree</code>, and if we train a model with <code>(input:cat,output:tree)</code> and <code>(input:kitten,output:tree)</code> this will eventually force the model to understand that, <code>cat </code>and <code>kitten </code>both as related to <code>tree</code>, thus placing <code>cat </code>and <code>kitten </code>closely in the embedding space. </p>
<h3>The model</h3>
<p>Now let’s see how exactly we make this work with Skip-gram model. Also let us define the following Notation<br>
V – Vocabulary Size (Number of unique words in the corpora)<br>
P – The Projection or the Embedding Layer<br>
D – Dimensionality of the Embedding Space<br>
b – Size of a single Bach</p>
<p>The model we are assuming is a simple logistic regression (Softmax) model. So the high-level architecture looks like follows. I have included 2 diagrams, where the one on the left is the conceptual architecture, where the one on the right is the implemented architectures. Though the architectures is different, it should be noted it does not make any changes to functionality.</p>
<p><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/skip_gram.png" alt="Conceptual and Implemented Diagrams of Skip-gram" data-recalc-dims="1" class="jetpack-lazy-image--handled" data-lazy-loaded="1"><noscript><img src="https://i1.wp.com/www.thushv.com/wp-content/uploads/2016/12/skip_gram.png" alt="Conceptual and Implemented Diagrams of Skip-gram" data-recalc-dims="1" /></noscript></p>
<p>Let’s talk about the differences in two views of the model. To make things obvious, let us assume the sentence <code>The dog barked at the mailman</code>. We can visualize the first model as a model that is being trained on data such as <code>(input:'dog',output:['the','barked','at','the','mailman'])</code> while sharing weights and biases of the softmax layer. In other words, the conceptual model trains multiple outputs to the same input simultaneously. However, this is practically difficult to implement. A more practical solution would be to break the above tuple <code>(input:'dog', output:['the','barked','at','the','mailman'])</code> to several single (input,output) tuples as <code>(input:'dog', output:'the'),(input:'dog', output:'barked'),...,(input:'dog', output:'mailman')</code>. This is what is illustrated in the right-sided model.</p>
<h3>Skip-gram Model</h3>
<h4>Intuition (Data Generation)</h4>
<p>First here’s the intuition for the Skip-gram model through an example. Consider the sentence, <code>The dog barked at the mailman</code>. First, we pick a word from a text (<code>dog</code>), then we define a window called <code>skip_window</code> which is the number of words back and forth from the selected word we are going to look at. For example if <code>skip_window = 2</code> then, <code>['The','dog','barked','at']</code> will be inside the window. Also, let us assume the definition that <code>skip_window</code> assumes only one-side from the selected word, where <code>span</code> is the whole window. Let us now define another parameter called <code>num_skips</code> denoting the number of different output words we will pick within the span for a single word. Now assuming <code>skip_window=2</code> and <code>num_skips=2</code> we compose the following (input,output) tuples. <code>('dog','barked'),('dog','the')</code>. </p>
<p>I will not delve into specific details of batch generation as this is not the most important part of the model. Please refer functions <code>read_data</code>, <code>build_dataset</code> and <code>generate_batch</code> from <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb" target="_blank">5_word2vec.ipynb</a> for generating batches of data. Also you will be able to find rigorously commented completed assignment from my Github account (<a href="https://github.com/thushv89/udacity_deeplearning_complete/blob/master/5_word2vec.py" target="_blank">5_word2vec.py</a> and <a href="https://github.com/thushv89/udacity_deeplearning_complete/blob/master/5_word2vec_CBOW.ipynb" target="_blank">5_word2vec_cbow.ipynb</a>).</p>
<p>The important thing to understand is that, these functions will convert textual words to a numerical representations. In other words, it will assign a unique “ID” to unique words. For example, if the sentence The dog barked at the mailman is fed to the functions, they will output [1,2,3,1,4] as the output where id(‘the’)=1, id(‘dog’)=2, etc.</p>
<h4> Training the Model </h4>
<p>Now, we have (input,output) tuples without using any labeled data, thus Unsupervised learning. Now all there left to do is train the model with produced inputs and outputs. I will explain how to implement skip-gram model with tensorflow.</p>
<p>First we define the required input,output and other required Tensors and parameter values. This should be self explanatory if you are slightly familiar with how tensorflow works.</p>
<div><div id="highlighter_979897" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><div class="line number18 index17 alt1">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1">22</div><div class="line number23 index22 alt2">23</div><div class="line number24 index23 alt1">24</div><div class="line number25 index24 alt2">25</div><div class="line number26 index25 alt1">26</div><div class="line number27 index26 alt2">27</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python keyword">if</code> <code class="python plain">__name__ </code><code class="python keyword">=</code><code class="python keyword">=</code> <code class="python string">'__main__'</code><code class="python plain">:</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">batch_size </code><code class="python keyword">=</code> <code class="python value">128</code></div><div class="line number3 index2 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">embedding_size </code><code class="python keyword">=</code> <code class="python value">128</code></div><div class="line number4 index3 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">skip_window </code><code class="python keyword">=</code> <code class="python value">1</code> <code class="python comments"># How many words to consider left and right.</code></div><div class="line number5 index4 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">num_skips </code><code class="python keyword">=</code> <code class="python value">2</code> <code class="python comments"># How many times to reuse an input to generate a label.</code></div><div class="line number6 index5 alt1">&nbsp;</div><div class="line number7 index6 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_size </code><code class="python keyword">=</code> <code class="python value">16</code> <code class="python comments"># Random set of words to evaluate similarity on.</code></div><div class="line number8 index7 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_window </code><code class="python keyword">=</code> <code class="python value">100</code></div><div class="line number9 index8 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent </code></div><div class="line number10 index9 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_examples </code><code class="python keyword">=</code> <code class="python plain">np.array(random.sample(</code><code class="python functions">range</code><code class="python plain">(valid_window), valid_size</code><code class="python keyword">/</code><code class="python keyword">/</code><code class="python value">2</code><code class="python plain">))</code></div><div class="line number11 index10 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_examples </code><code class="python keyword">=</code> <code class="python plain">np.append(valid_examples,random.sample(</code><code class="python functions">range</code><code class="python plain">(</code><code class="python value">1000</code><code class="python plain">,</code><code class="python value">1000</code><code class="python keyword">+</code><code class="python plain">valid_window), valid_size</code><code class="python keyword">/</code><code class="python keyword">/</code><code class="python value">2</code><code class="python plain">))</code></div><div class="line number12 index11 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">num_sampled </code><code class="python keyword">=</code> <code class="python value">64</code> <code class="python comments"># Number of negative examples to sample for sampeled_softmax.</code></div><div class="line number13 index12 alt2">&nbsp;</div><div class="line number14 index13 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">graph </code><code class="python keyword">=</code> <code class="python plain">tf.Graph()</code></div><div class="line number15 index14 alt2">&nbsp;</div><div class="line number16 index15 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">with graph.as_default(), tf.device(</code><code class="python string">'/cpu:0'</code><code class="python plain">):</code></div><div class="line number17 index16 alt2">&nbsp;</div><div class="line number18 index17 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># Input data.</code></div><div class="line number19 index18 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">train_dataset </code><code class="python keyword">=</code> <code class="python plain">tf.placeholder(tf.int32, shape</code><code class="python keyword">=</code><code class="python plain">[batch_size])</code></div><div class="line number20 index19 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">train_labels </code><code class="python keyword">=</code> <code class="python plain">tf.placeholder(tf.int32, shape</code><code class="python keyword">=</code><code class="python plain">[batch_size, </code><code class="python value">1</code><code class="python plain">])</code></div><div class="line number21 index20 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_dataset </code><code class="python keyword">=</code> <code class="python plain">tf.constant(valid_examples, dtype</code><code class="python keyword">=</code><code class="python plain">tf.int32)</code></div><div class="line number22 index21 alt1">&nbsp;</div><div class="line number23 index22 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># Variables.</code></div><div class="line number24 index23 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">embeddings </code><code class="python keyword">=</code> <code class="python plain">tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], </code><code class="python keyword">-</code><code class="python value">1.0</code><code class="python plain">, </code><code class="python value">1.0</code><code class="python plain">))</code></div><div class="line number25 index24 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">softmax_weights </code><code class="python keyword">=</code> <code class="python plain">tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],</code></div><div class="line number26 index25 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">stddev</code><code class="python keyword">=</code><code class="python value">1.0</code> <code class="python keyword">/</code> <code class="python plain">math.sqrt(embedding_size)))</code></div><div class="line number27 index26 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">softmax_biases </code><code class="python keyword">=</code> <code class="python plain">tf.Variable(tf.zeros([vocabulary_size]))</code></div></div></td></tr></tbody></table></div></div>
<p>Now we define symbolic operations. First embedding_lookup is used to look up corresponding embeddings of the inputs. In other words, the embedding layer <code>P</code>, is of size VxD, which will contain embedding vectors (<code>D</code> dimensional) for all the words in the vocabulary. In order to train the model for a single instance you need to find the corresponding embedding vectors for the given input words by an id lookup (<code>train_dataset</code> in this case contain a set of unique ids corresponding to each word in the batch of data).  Although it possible to do this manually, use of this function is required as tensorflow doesn’t allow index lookup with Tensors.</p>
<div><div id="highlighter_988761" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python plain">embed </code><code class="python keyword">=</code> <code class="python plain">tf.nn.embedding_lookup(embeddings, train_dataset)</code></div></div></td></tr></tbody></table></div></div>
<p>Next, we use an altered version of softmax to calculate the loss. Since the vocabulary can be very large (~50000) for a standard text file it is computationally wasteful to calculate full softmax loss. Therefore, we sample <code>num_sampled</code> number of negative softmax units from full set of V softmax units (units that should have 0 as their output) and calculate the loss only with them. This is found to be very effective approximation to the full softmax but with increased performance.</p>
<div><div id="highlighter_371365" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python plain">loss </code><code class="python keyword">=</code> <code class="python plain">tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">train_labels, num_sampled, vocabulary_size))</code></div></div></td></tr></tbody></table></div></div>
<p>Now we use an advance gradient optimization technique called Adagrad which allows you to find “needle in the haystack”. This works better than standard GD because, Adagrad works very well when there are lot of variables to optimize (<code>softmax_weights</code>, <code>softmax_biases</code> and <code>embed</code>). And mind you all these variables are of 1,000,000 scale.</p>
<div><div id="highlighter_630590" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python plain">optimizer </code><code class="python keyword">=</code> <code class="python plain">tf.train.AdagradOptimizer(</code><code class="python value">1.0</code><code class="python plain">).minimize(loss)</code></div></div></td></tr></tbody></table></div></div>
<p>This code snippet calculates the similarity (i.e. cosine distance) between a given minibatch of words against all the words, using the learnt embeddings.</p>
<div><div id="highlighter_786608" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python comments"># We use the cosine distance:</code></div><div class="line number2 index1 alt1"><code class="python plain">norm </code><code class="python keyword">=</code> <code class="python plain">tf.sqrt(tf.reduce_sum(tf.square(embeddings), </code><code class="python value">1</code><code class="python plain">, keep_dims</code><code class="python keyword">=</code><code class="python color1">True</code><code class="python plain">))</code></div><div class="line number3 index2 alt2"><code class="python plain">normalized_embeddings </code><code class="python keyword">=</code> <code class="python plain">embeddings </code><code class="python keyword">/</code> <code class="python plain">norm</code></div><div class="line number4 index3 alt1"><code class="python plain">valid_embeddings </code><code class="python keyword">=</code> <code class="python plain">tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)</code></div><div class="line number5 index4 alt2"><code class="python plain">similarity </code><code class="python keyword">=</code> <code class="python plain">tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))</code></div></div></td></tr></tbody></table></div></div>
<p>Now, all the required things nicely defined, all we have to do is feed the data to the placeholders and run the optimizer, which will minimize the loss with respect to the parameters (<code>softmax_weights</code>, <code>softmax_biases</code> and <code>embeds</code>). Then we also run the loss operation to print out the average loss. This is important as we can see if something is wrong if the loss is behaving weird (e.g. negative or increasing with time).</p>
<div><div id="highlighter_322552" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python plain">with tf.Session(graph</code><code class="python keyword">=</code><code class="python plain">graph) as session:</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">tf.initialize_all_variables().run()</code></div><div class="line number3 index2 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code><code class="python plain">(</code><code class="python string">'Initialized'</code><code class="python plain">)</code></div><div class="line number4 index3 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">average_loss </code><code class="python keyword">=</code> <code class="python value">0</code></div><div class="line number5 index4 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">step </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(num_steps):</code></div><div class="line number6 index5 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">batch_data, batch_labels </code><code class="python keyword">=</code> <code class="python plain">generate_batch(batch_size, num_skips, skip_window)</code></div><div class="line number7 index6 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">feed_dict </code><code class="python keyword">=</code> <code class="python plain">{train_dataset : batch_data, train_labels : batch_labels}</code></div><div class="line number8 index7 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">_, l </code><code class="python keyword">=</code> <code class="python plain">session.run([optimizer, loss], feed_dict</code><code class="python keyword">=</code><code class="python plain">feed_dict)</code></div><div class="line number9 index8 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">average_loss </code><code class="python keyword">+</code><code class="python keyword">=</code> <code class="python plain">l</code></div><div class="line number10 index9 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">if</code> <code class="python plain">step </code><code class="python keyword">%</code> <code class="python value">2000</code> <code class="python keyword">=</code><code class="python keyword">=</code> <code class="python value">0</code><code class="python plain">:</code></div><div class="line number11 index10 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">if</code> <code class="python plain">step &gt; </code><code class="python value">0</code><code class="python plain">:</code></div><div class="line number12 index11 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">average_loss </code><code class="python keyword">=</code> <code class="python plain">average_loss </code><code class="python keyword">/</code> <code class="python value">2000</code></div><div class="line number13 index12 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># The average loss is an estimate of the loss over the last 2000 batches.</code></div><div class="line number14 index13 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code><code class="python plain">(</code><code class="python string">'Average loss at step %d: %f'</code> <code class="python keyword">%</code> <code class="python plain">(step, average_loss))</code></div><div class="line number15 index14 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">average_loss </code><code class="python keyword">=</code> <code class="python value">0</code></div></div></td></tr></tbody></table></div></div>
<p>Now, every 10000 <code>steps</code>, we evaluate our model by looking the most similar <code>top_k</code> words to the data in the <code>valid_dataset</code>.</p>
<div><div id="highlighter_91726" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># note that this is expensive (~20% slowdown if computed every 500 steps)</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">if</code> <code class="python plain">step </code><code class="python keyword">%</code> <code class="python value">10000</code> <code class="python keyword">=</code><code class="python keyword">=</code> <code class="python value">0</code><code class="python plain">:</code></div><div class="line number3 index2 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">sim </code><code class="python keyword">=</code> <code class="python plain">similarity.</code><code class="python functions">eval</code><code class="python plain">()</code></div><div class="line number4 index3 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">i </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(valid_size):</code></div><div class="line number5 index4 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">valid_word </code><code class="python keyword">=</code> <code class="python plain">reverse_dictionary[valid_examples[i]]</code></div><div class="line number6 index5 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">top_k </code><code class="python keyword">=</code> <code class="python value">8</code> <code class="python comments"># number of nearest neighbors</code></div><div class="line number7 index6 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">nearest </code><code class="python keyword">=</code> <code class="python plain">(</code><code class="python keyword">-</code><code class="python plain">sim[i, :]).argsort()[</code><code class="python value">1</code><code class="python plain">:top_k</code><code class="python keyword">+</code><code class="python value">1</code><code class="python plain">]</code></div><div class="line number8 index7 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">log </code><code class="python keyword">=</code> <code class="python string">'Nearest to %s:'</code> <code class="python keyword">%</code> <code class="python plain">valid_word</code></div><div class="line number9 index8 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">k </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(top_k):</code></div><div class="line number10 index9 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">close_word </code><code class="python keyword">=</code> <code class="python plain">reverse_dictionary[nearest[k]]</code></div><div class="line number11 index10 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">log </code><code class="python keyword">=</code> <code class="python string">'%s %s,'</code> <code class="python keyword">%</code> <code class="python plain">(log, close_word)</code></div><div class="line number12 index11 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code><code class="python plain">(log)</code></div><div class="line number13 index12 alt2"><code class="python plain">final_embeddings </code><code class="python keyword">=</code> <code class="python plain">normalized_embeddings.</code><code class="python functions">eval</code><code class="python plain">()</code></div></div></td></tr></tbody></table></div></div>
<h4>Results</h4>
<p>Now if you do this properly, you should see improvements of word similarity over time. Here are the results I got at 0th step and after 100,000 steps.</p>
<div><div id="highlighter_703859" class="syntaxhighlighter  plain"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><div class="line number18 index17 alt1">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1">22</div><div class="line number23 index22 alt2">23</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="plain plain">==========================================================================================================</code></div><div class="line number2 index1 alt1"><code class="plain plain">Average loss at step 0: 7.805069</code></div><div class="line number3 index2 alt2"><code class="plain plain">Nearest to when: drummer, acceleration, compost, loan, severe, quicker, nite, pies,</code></div><div class="line number4 index3 alt1"><code class="plain plain">Nearest to people: hr, infertile, detractors, programmability, capacitors, lounge, report, horn,</code></div><div class="line number5 index4 alt2"><code class="plain plain">Nearest to american: department, corvettes, rejoining, lindy, laconic, wels, kojiki, bibliography,</code></div><div class="line number6 index5 alt1"><code class="plain plain">Nearest to than: dallas, corrino, whispers, empowered, intakes, homer, salvage, fern,</code></div><div class="line number7 index6 alt2"><code class="plain plain">...</code></div><div class="line number8 index7 alt1"><code class="plain plain">Nearest to except: finite, altitudes, particular, helper, endeavoured, scenes, helaman, myocardium,</code></div><div class="line number9 index8 alt2"><code class="plain plain">Nearest to report: occupants, costing, brooker, armas, adversaries, powering, hawkwind, people,</code></div><div class="line number10 index9 alt1"><code class="plain plain">Nearest to professional: bronx, covalently, reappeared, inti, anthologies, alaska, described, midwestern,</code></div><div class="line number11 index10 alt2"><code class="plain plain">Nearest to bbc: cruzi, galatia, football, grammaticus, tights, homilies, agonists, turbines,</code></div><div class="line number12 index11 alt1"><code class="plain plain">==========================================================================================================</code></div><div class="line number13 index12 alt2"><code class="plain plain">Average loss at step 100000: 3.359176</code></div><div class="line number14 index13 alt1"><code class="plain plain">Nearest to when: if, before, while, although, where, after, though, because,</code></div><div class="line number15 index14 alt2"><code class="plain plain">Nearest to people: children, students, players, individuals, men, adapting, women, americans,</code></div><div class="line number16 index15 alt1"><code class="plain plain">Nearest to american: british, australian, german, french, italian, scottish, canadian, soccer,</code></div><div class="line number17 index16 alt2"><code class="plain plain">Nearest to than: or, much, announcements, and, leningrad, spark, kish, while,</code></div><div class="line number18 index17 alt1"><code class="plain plain">...</code></div><div class="line number19 index18 alt2"><code class="plain plain">Nearest to except: especially, embodied, endeavoured, scenes, devonshire, every, indoors, example,</code></div><div class="line number20 index19 alt1"><code class="plain plain">Nearest to report: sloop, woodbridge, costing, pit, occupants, atheism, jannaeus, uns,</code></div><div class="line number21 index20 alt2"><code class="plain plain">Nearest to professional: anthologies, major, cumings, inti, reset, hollow, copyrighted, covalently,</code></div><div class="line number22 index21 alt1"><code class="plain plain">Nearest to bbc: paper, galatia, fliers, flavia, earth, manufacturing, icosahedron, grammaticus,</code></div><div class="line number23 index22 alt2"><code class="plain plain">==========================================================================================================</code></div></div></td></tr></tbody></table></div></div>
<p>This is it for the skip-gram model.<br>
Next: <a href="http://www.thushv.com/natural_language_processing/word2vec-part-2-nlp-with-deep-learning-with-tensorflow-cbow/" target="_blank">Continuous Bag of Words Model</a></p>
<h2>New Word2Vec technique: GloVe</h2>
<p>If you’re interested about new Word2Vec techniques, I’m putting the link to a new Word2Vec technique that saw the light recently.<br>
<a href="http://www.thushv.com/natural_language_processing/glove-global-vectors-for-word-representation/"><strong>GloVe: Global Vectors for Word Representation</strong></a></p>
				</div>
				<div class="single-container-tags">
					Tags: <a href="http://www.thushv.com/tag/bag-of-words/" rel="tag">bag of words</a>, <a href="http://www.thushv.com/tag/cbow/" rel="tag">cbow</a>, <a href="http://www.thushv.com/tag/deep-learning/" rel="tag">deep learning</a>, <a href="http://www.thushv.com/tag/nlp/" rel="tag">nlp</a>, <a href="http://www.thushv.com/tag/tensorflow/" rel="tag">tensorflow</a>				</div>
							</div>
				<hr>
			<div class="single-container-main-inner p-4">
				<div class="related-posts-title">
					<h2> You might also like </h2>
				</div>

			<div class="row">
			<!-- mt-2 class is bootstrap margin settter t means top and 2 is the spacing -->
<!-- pt-2 class is bootstrap padding settter t means top and 2 is the spacing -->
<div class="blog-post col-xs-12 col-sm-12 col-md-12 col-lg-12 col-xl-4">
	<div class="card mb-2">
		<div class="card-body">
			<!--img class="card-img-top flex-auto d-none d-md-block" data-src="holder.js/200x250?theme=thumb" style="width: 100%;" src="data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%22200%22%20height%3D%22250%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20200%20250%22%20preserveAspectRatio%3D%22none%22%3E%3Cdefs%3E%3Cstyle%20type%3D%22text%2Fcss%22%3E%23holder_162b97af9f5%20text%20%7B%20fill%3A%23eceeef%3Bfont-weight%3Abold%3Bfont-family%3AArial%2C%20Helvetica%2C%20Open%20Sans%2C%20sans-serif%2C%20monospace%3Bfont-size%3A13pt%20%7D%20%3C%2Fstyle%3E%3C%2Fdefs%3E%3Cg%20id%3D%22holder_162b97af9f5%22%3E%3Crect%20width%3D%22200%22%20height%3D%22250%22%20fill%3D%22%2355595c%22%3E%3C%2Frect%3E%3Cg%3E%3Ctext%20x%3D%2256.203125%22%20y%3D%22131%22%3EThumbnail%3C%2Ftext%3E%3C%2Fg%3E%3C%2Fg%3E%3C%2Fsvg%3E" data-holder-rendered="true"-->
			<img class="card-img-top flex-auto d-none d-md-block" style="width: 100%;" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/font-705667_640.jpg">
			<h3 class=" card-header post-title ">Light on Math Machine Learning: Intuitive Guide to Understanding Word2vec</h3>
			<!--p class="card-text blog-post-meta">June 5, 2018</p-->
			<p class="card-text lead">Here comes the third blog post in the series of light on math machine learning A-Z. This article is going to be about Word2vec algorithms. Word2vec algorithms output word vectors. Word vectors, underpin many of the natural language processing (NLP) systems, that have taken the world by a storm (Amazon...</p>
			
			<div class="d-flex justify-content-center pb-2">
				<a target="_blank" href="http://www.thushv.com/natural_language_processing/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec/"><button type="button" class="btn btn-md btn-outline-secondary">Read More</button></a>
			</div>
		</div>
	</div>
</div><!-- mt-2 class is bootstrap margin settter t means top and 2 is the spacing -->
<!-- pt-2 class is bootstrap padding settter t means top and 2 is the spacing -->
<div class="blog-post col-xs-12 col-sm-12 col-md-12 col-lg-12 col-xl-4">
	<div class="card mb-2">
		<div class="card-body">
			<!--img class="card-img-top flex-auto d-none d-md-block" data-src="holder.js/200x250?theme=thumb" style="width: 100%;" src="data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%22200%22%20height%3D%22250%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20200%20250%22%20preserveAspectRatio%3D%22none%22%3E%3Cdefs%3E%3Cstyle%20type%3D%22text%2Fcss%22%3E%23holder_162b97af9f5%20text%20%7B%20fill%3A%23eceeef%3Bfont-weight%3Abold%3Bfont-family%3AArial%2C%20Helvetica%2C%20Open%20Sans%2C%20sans-serif%2C%20monospace%3Bfont-size%3A13pt%20%7D%20%3C%2Fstyle%3E%3C%2Fdefs%3E%3Cg%20id%3D%22holder_162b97af9f5%22%3E%3Crect%20width%3D%22200%22%20height%3D%22250%22%20fill%3D%22%2355595c%22%3E%3C%2Frect%3E%3Cg%3E%3Ctext%20x%3D%2256.203125%22%20y%3D%22131%22%3EThumbnail%3C%2Ftext%3E%3C%2Fg%3E%3C%2Fg%3E%3C%2Fsvg%3E" data-holder-rendered="true"-->
			<img class="card-img-top flex-auto d-none d-md-block" style="width: 100%;" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/connection-2924266_640.jpg">
			<h3 class=" card-header post-title ">Light on Math Machine Learning: Intuitive Guide to Convolution Neural Networks</h3>
			<!--p class="card-text blog-post-meta">May 31, 2018</p-->
			<p class="card-text lead">This is the second article on my series introducing machine learning concepts with while stepping very lightly on mathematics. If you missed previous article you can find in here. Fun fact, I’m going to make this an interesting adventure by introducing some machine learning concept for every letter in the...</p>
			
			<div class="d-flex justify-content-center pb-2">
				<a target="_blank" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/"><button type="button" class="btn btn-md btn-outline-secondary">Read More</button></a>
			</div>
		</div>
	</div>
</div><!-- mt-2 class is bootstrap margin settter t means top and 2 is the spacing -->
<!-- pt-2 class is bootstrap padding settter t means top and 2 is the spacing -->
<div class="blog-post col-xs-12 col-sm-12 col-md-12 col-lg-12 col-xl-4">
	<div class="card mb-2">
		<div class="card-body">
			<!--img class="card-img-top flex-auto d-none d-md-block" data-src="holder.js/200x250?theme=thumb" style="width: 100%;" src="data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%22200%22%20height%3D%22250%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20200%20250%22%20preserveAspectRatio%3D%22none%22%3E%3Cdefs%3E%3Cstyle%20type%3D%22text%2Fcss%22%3E%23holder_162b97af9f5%20text%20%7B%20fill%3A%23eceeef%3Bfont-weight%3Abold%3Bfont-family%3AArial%2C%20Helvetica%2C%20Open%20Sans%2C%20sans-serif%2C%20monospace%3Bfont-size%3A13pt%20%7D%20%3C%2Fstyle%3E%3C%2Fdefs%3E%3Cg%20id%3D%22holder_162b97af9f5%22%3E%3Crect%20width%3D%22200%22%20height%3D%22250%22%20fill%3D%22%2355595c%22%3E%3C%2Frect%3E%3Cg%3E%3Ctext%20x%3D%2256.203125%22%20y%3D%22131%22%3EThumbnail%3C%2Ftext%3E%3C%2Fg%3E%3C%2Fg%3E%3C%2Fsvg%3E" data-holder-rendered="true"-->
			<img class="card-img-top flex-auto d-none d-md-block" style="width: 100%;" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/globe-110775_1280.jpg">
			<h3 class=" card-header post-title ">Neural Machine Translator with 50 Lines of Code + Guide</h3>
			<!--p class="card-text blog-post-meta">December 27, 2017</p-->
			<p class="card-text lead">Jupyter Notebook for this Tutorial: Here Recently, I had to take a dive into the seq2seq library of TensorFlow. And I wanted to a quick intro to the library for the purpose of implementing a Neural Machine Translator (NMT). I simply wanted to know “what do I essentially need to...</p>
			
			<div class="d-flex justify-content-center pb-2">
				<a target="_blank" href="http://www.thushv.com/natural_language_processing/neural-machine-translator-with-50-lines-of-code-using-tensorflow-seq2seq/"><button type="button" class="btn btn-md btn-outline-secondary">Read More</button></a>
			</div>
		</div>
	</div>
</div>			</div> <!-- related-posts-outer -->
			</div>
						
			</div> <!-- col -->
			<div class="col-2 social-widget">
				<nav class="navbar sticky-top flex-column" role="navigation">	
					<h5 class="card-header d-none d-lg-block">Share</h5><div class="social-item-group"><div class="social-item"><a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F" target="_blank"><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/facebook_sm.png"></a></div><div class="social-item"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F&amp;title=Word2Vec%20(Part%201):%20NLP%20With%20Deep%20Learning%20with%20Tensorflow%20(Skip-gram)" target="_blank"><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/linkedin_sm.png"></a></div><div class="social-item"><a href="https://plus.google.com/share?url=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F" target="_blank"><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/google_sm.png"></a></div><div class="social-item"><a href="https://twitter.com/intent/tweet?text=Word2Vec%20(Part%201):%20NLP%20With%20Deep%20Learning%20with%20Tensorflow%20(Skip-gram)&amp;url=http%3A%2F%2Fwww.thushv.com%2Fnatural_language_processing%2Fword2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram%2F&amp;via=thushv" target="_blank"><img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/twitter_sm.png"></a></div></div>				</nav>
			</div>
			</div>
		</div>
					
</div>
<hr>

<div id="disqus_thread"></div>
</div> <!-- div id=container -->

<!-- jQuery first, then Tether, then Bootstrap JS. -->
	<!-- jquery.slim.min.js affects the online cv page -->
	<footer class="blog-footer"></footer>
    <script src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/jquery.min.js.下载" crossorigin="anonymous"></script>
    <script src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/tether.min.js.下载" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/bootstrap.min.js.下载" crossorigin="anonymous"></script>
	

	<div style="display:none">
	</div>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/shCore.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/shBrushPython.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/shBrushPlain.js.下载"></script>
<script type="text/javascript">
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "http://www.thushv.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.getElementsByTagName("head")[0].insertBefore( corecss, document.getElementById("syntaxhighlighteranchor") );
		var themecssurl = "http://www.thushv.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		//document.getElementById("syntaxhighlighteranchor").appendChild(themecss);
		document.getElementsByTagName("head")[0].insertBefore( themecss, document.getElementById("syntaxhighlighteranchor") );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();
</script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/photon.min.js.下载"></script>
<script type="text/javascript">
/* <![CDATA[ */
var countVars = {"disqusShortname":"www-thushv-com"};
/* ]]> */
</script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/comment_count.js.下载"></script>
<script type="text/javascript">
/* <![CDATA[ */
var embedVars = {"disqusConfig":{"integration":"wordpress 3.0.15"},"disqusIdentifier":"176 http:\/\/www.thushv.com\/?p=176","disqusShortname":"www-thushv-com","disqusTitle":"Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)","disqusUrl":"http:\/\/www.thushv.com\/natural_language_processing\/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram\/","postId":"176"};
/* ]]> */
</script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/comment_embed.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/devicepx-jetpack.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/gprofiles.js.下载"></script>
<script type="text/javascript">
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/wpgroho.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/lazy-images.min.js.下载"></script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/wp-embed.min.js.下载"></script>
<script type="text/javascript" id="slb_context">/* <![CDATA[ */if ( !!window.jQuery ) {(function($){$(document).ready(function(){if ( !!window.SLB ) { {$.extend(SLB, {"context":["public","user_guest"]});} }})})(jQuery);}/* ]]> */</script>
<script type="text/javascript" src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/e-201847.js.下载" async="async" defer="defer"></script>
<script type="text/javascript">
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:6.0',blog:'118787174',post:'176',tz:'0',srv:'www.thushv.com'} ]);
	_stq.push([ 'clickTrackerInit', '118787174', '176' ]);
</script>
	

<img src="./Thushv » Word2Vec (Part 1)_ NLP With Deep Learning with Tensorflow (Skip-gram)_files/g.gif" alt=":)" width="6" height="5" id="wpstats"></body></html>