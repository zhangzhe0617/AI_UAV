## 电影评论的情感预测
* 根据电影评论的评论文本和喜好标签来训练模型，最后实现根据文本预测喜好标签
* 采用rnn来代替普通前馈神经网络，即每条评论文本信息的每个单词当做前后时序数据输入，是否能否提高模型精度？
* 例如某条评论是200个单词，则按照把他当做200个时间步长输入模型
* 相比普通的前馈神经网络，只考虑了这200个单词是是否存在的问题，没有考虑时序，即前后语义上的理解

### 步骤：  
1、**读取文本数据 ，并清理数据**  
* 读取载入文本 `readline` `readlines` `read` 不同函数读取的格式不同
* 处理字符串，去掉标点符号

2、**对输入字符串进行向量化编码**
* 采用counter创建单词表
* 创建单词表索引
* 对输入文本字符串进行编码，即将文本转化成数字索引
* 对输入标签进行编码

3、**规范输入长度**
因为采用rnn，即需要统一时间长度，这里的时间长度就是指单词数量，则统一取前200个单词，超过200个单词的评论将被截取，少于200个单词的评论将在单词前前补零

4、**拆分训练集 测试集和验证集**
因为是多条评论数据，所有无需打乱顺序，按照顺序取即可

5、**创建网络**

* 首先需要**词嵌入** ，即采用**word2vec**将热独编码的高维向量转换成低维向量  
>例如数据是25000 * 200（20000条评论，每条评论取了200个词）  
>进行热独编码则为25000 * 200 * 70000（单词表总数70000个）  
>查找表变成了 25000 * 200 * 300 （嵌入层的向量维度300）

* 搭建网络 采用**LSTM搭建多层**  
> rnn的cell采用LSTM单元  
> LSTM的size即是四个控制门的数量，即代表隐层数量  
> **200个单词做前后的时序数据输入LSTM**

* 连接全连接层    


* batch迭代器
> 将25000行的训练数据进行分割，采用迭代器和生成器

6、**训练数据并测试精度**

```
tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)
lstm = tf.contrib.rnn.BasicLSTMCell(num_units)
drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)
cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)

``` 
### 结论
竟然和前面的采用前馈神经网络的情感分析模型差不多精度，耗时还很多，真是神奇！！！

